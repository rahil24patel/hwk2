{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bae1c0d-01f8-4a22-8288-b0c79b5b4dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MA_ROOT: /scion/5261/econ470001/ma-data/ma\n",
      "ENROLL_EXTRACTED exists: True\n",
      "SAREA_EXTRACTED exists: True\n",
      "PEN_EXTRACTED exists: True\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 140)\n",
    "\n",
    "YEARS = list(range(2014, 2020))\n",
    "\n",
    "CACHE_DIR = Path(\"data/cache\")\n",
    "OUT_DIR   = Path(\"data/processed\")\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHUNKSIZE = 300_000\n",
    "\n",
    "CANDIDATE_MA_ROOTS = [\n",
    "    Path(\"/scion/5261/econ470001/ma-data/ma\"),\n",
    "    Path(\"/econ470/a0/work/ma-data/ma\"),\n",
    "    Path.cwd().parent / \"ma-data\" / \"ma\",\n",
    "]\n",
    "\n",
    "def pick_existing(paths: list[Path]) -> Path | None:\n",
    "    for p in paths:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "MA_ROOT = pick_existing(CANDIDATE_MA_ROOTS)\n",
    "if MA_ROOT is None:\n",
    "    raise FileNotFoundError(\"Could not find ma-data/ma. Update CANDIDATE_MA_ROOTS if needed.\")\n",
    "\n",
    "ENROLL_EXTRACTED = MA_ROOT / \"enrollment\" / \"Extracted Data\"\n",
    "SAREA_EXTRACTED  = MA_ROOT / \"service-area\" / \"Extracted Data\"\n",
    "PEN_EXTRACTED    = MA_ROOT / \"penetration\" / \"Extracted Data\"\n",
    "\n",
    "print(\"MA_ROOT:\", MA_ROOT)\n",
    "print(\"ENROLL_EXTRACTED exists:\", ENROLL_EXTRACTED.exists())\n",
    "print(\"SAREA_EXTRACTED exists:\", SAREA_EXTRACTED.exists())\n",
    "print(\"PEN_EXTRACTED exists:\", PEN_EXTRACTED.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e862bdd-f30c-4f3c-b9c6-67ad7a364126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_colname(s: str) -> str:\n",
    "    s = str(s).strip().lower()\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-z0-9_]\", \"\", s)\n",
    "    return s\n",
    "\n",
    "def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [norm_colname(c) for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def first_existing_col(cols: list[str], candidates: list[str]) -> str | None:\n",
    "    s = set(cols)\n",
    "    for c in candidates:\n",
    "        if c in s:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def coerce_id_series(x: pd.Series, width: int | None = None) -> pd.Series:\n",
    "    s = x.astype(str).str.strip()\n",
    "    s = s.replace({\"nan\": np.nan, \"None\": np.nan, \"\": np.nan})\n",
    "    s = s.str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "    if width is not None:\n",
    "        s = s.str.zfill(width)\n",
    "    return s\n",
    "\n",
    "def to_numeric_clean(x: pd.Series) -> pd.Series:\n",
    "    s = x.astype(str).str.replace(\",\", \"\", regex=False).str.strip()\n",
    "    s = s.replace({\"nan\": np.nan, \"None\": np.nan, \"\": np.nan, \"*\": np.nan, \"$-\": np.nan, \"-\": np.nan})\n",
    "    s = s.str.replace(\"$\", \"\", regex=False)\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "def _digits_zfill(x: pd.Series, width: int) -> pd.Series:\n",
    "    s = x.astype(str).str.strip()\n",
    "    s = s.replace({\"nan\": np.nan, \"None\": np.nan, \"\": np.nan})\n",
    "    s = s.str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "    s = s.str.replace(r\"[^0-9]\", \"\", regex=True)\n",
    "    s = s.where(s.str.len() > 0, np.nan)\n",
    "    return s.str.zfill(width)\n",
    "\n",
    "def coerce_fips(df: pd.DataFrame) -> pd.Series:\n",
    "    cols = list(df.columns)\n",
    "\n",
    "    fips_col = first_existing_col(\n",
    "        cols,\n",
    "        [\n",
    "            \"fips\",\n",
    "            \"fips_state_county_code\",\n",
    "            \"fipsstatecountycode\",\n",
    "            \"county_fips\",\n",
    "            \"countyfips\",\n",
    "            \"cnty_fips\",\n",
    "            \"fipscounty\",\n",
    "            \"fipscnty\",\n",
    "        ],\n",
    "    )\n",
    "    if fips_col is not None:\n",
    "        return _digits_zfill(df[fips_col], 5)\n",
    "\n",
    "    st_col = first_existing_col(cols, [\"state_fips\", \"statefips\", \"fipsst\", \"fips_state\"])\n",
    "    ct_col = first_existing_col(cols, [\"county_fips\", \"countyfips\", \"fipscnty\", \"fips_county\"])\n",
    "\n",
    "    if st_col is not None and ct_col is not None:\n",
    "        st = _digits_zfill(df[st_col], 2)\n",
    "        ct = _digits_zfill(df[ct_col], 3)\n",
    "        out = (st.fillna(\"\") + ct.fillna(\"\")).replace({\"\": np.nan})\n",
    "        return out\n",
    "\n",
    "    return pd.Series([np.nan] * len(df), index=df.index, dtype=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "808e83b1-c08c-40cf-b922-7ef219c4aaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_month_file(root: Path, year: int, month: int, kind: str) -> Path:\n",
    "    m2 = f\"{month:02d}\"\n",
    "    files = [p for p in root.rglob(\"*\") if p.is_file()]\n",
    "\n",
    "    if kind == \"enroll\":\n",
    "        pats = [rf\".*{year}.*{m2}.*enroll.*\\.(csv|txt)$\", rf\".*enroll.*{year}.*{m2}.*\\.(csv|txt)$\"]\n",
    "    elif kind == \"sarea\":\n",
    "        pats = [\n",
    "            rf\".*{year}.*{m2}.*(service|sa|sarea|cnty_sa|service_area).*?\\.(csv|txt)$\",\n",
    "            rf\".*(service|sa|sarea|cnty_sa|service_area).*{year}.*{m2}.*?\\.(csv|txt)$\",\n",
    "        ]\n",
    "    else:\n",
    "        raise ValueError(\"unknown kind\")\n",
    "\n",
    "    for pat in pats:\n",
    "        rx = re.compile(pat, flags=re.I)\n",
    "        hits = [p for p in files if rx.match(p.name)]\n",
    "        if hits:\n",
    "            hits.sort(key=lambda p: len(p.name))\n",
    "            return hits[0]\n",
    "\n",
    "    rx = re.compile(rf\".*{year}.*{m2}.*\\.(csv|txt)$\", flags=re.I)\n",
    "    hits = [p for p in files if rx.match(p.name)]\n",
    "    if hits:\n",
    "        hits.sort(key=lambda p: len(p.name))\n",
    "        return hits[0]\n",
    "\n",
    "    raise FileNotFoundError(f\"Could not find {kind} file for {year}-{m2} under {root}\")\n",
    "\n",
    "def find_year_file(root: Path, year: int, kind: str) -> Path:\n",
    "    files = [p for p in root.rglob(\"*\") if p.is_file()]\n",
    "    if kind != \"penetration\":\n",
    "        raise ValueError(\"unknown kind\")\n",
    "\n",
    "    pats = [rf\".*{year}.*penet.*\\.(csv|txt)$\", rf\".*penet.*{year}.*\\.(csv|txt)$\"]\n",
    "    for pat in pats:\n",
    "        rx = re.compile(pat, flags=re.I)\n",
    "        hits = [p for p in files if rx.match(p.name)]\n",
    "        if hits:\n",
    "            hits.sort(key=lambda p: len(p.name))\n",
    "            return hits[0]\n",
    "\n",
    "    rx = re.compile(rf\".*{year}.*\\.(csv|txt)$\", flags=re.I)\n",
    "    hits = [p for p in files if rx.match(p.name)]\n",
    "    if hits:\n",
    "        hits.sort(key=lambda p: len(p.name))\n",
    "        return hits[0]\n",
    "\n",
    "    raise FileNotFoundError(f\"Could not find {kind} file for {year} under {root}\")\n",
    "\n",
    "def read_csv_any(path: Path, usecols=None) -> pd.DataFrame:\n",
    "    return pd.read_csv(\n",
    "        path,\n",
    "        dtype=str,\n",
    "        na_values=[\"*\", \"NA\", \"N/A\", \"\"],\n",
    "        keep_default_na=True,\n",
    "        encoding_errors=\"replace\",\n",
    "        low_memory=False,\n",
    "        usecols=usecols,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb1f24ff-77b2-4db3-953c-210dafd21712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_enroll_min(path: Path) -> pd.DataFrame:\n",
    "    want = {\n",
    "        \"contract_number\",\n",
    "        \"contract_id\",\n",
    "        \"plan_id\",\n",
    "        \"fips_state_county_code\",\n",
    "        \"enrollment\",\n",
    "        \"snp\",\n",
    "        \"snp_type\",\n",
    "        \"partc\",\n",
    "        \"part_c\",\n",
    "        \"plan_type\",\n",
    "        \"contract_type\",\n",
    "    }\n",
    "\n",
    "    df = read_csv_any(path, usecols=lambda c: norm_colname(c) in want)\n",
    "    df = normalize_columns(df)\n",
    "\n",
    "    contract_col = first_existing_col(list(df.columns), [\"contract_id\", \"contract_number\"])\n",
    "    if contract_col is None:\n",
    "        raise KeyError(\"Enrollment file missing contract column\")\n",
    "\n",
    "    df[\"contract_id\"] = coerce_id_series(df[contract_col], None)\n",
    "    df[\"plan_id\"] = coerce_id_series(df[\"plan_id\"], 3)\n",
    "    df[\"fips\"] = _digits_zfill(df.get(\"fips_state_county_code\", pd.Series([np.nan] * len(df))), 5)\n",
    "\n",
    "    df[\"enrollment\"] = to_numeric_clean(df[\"enrollment\"])\n",
    "\n",
    "    keep = [\"contract_id\", \"plan_id\", \"fips\", \"enrollment\"]\n",
    "    for c in [\"snp\", \"snp_type\", \"partc\", \"part_c\", \"plan_type\", \"contract_type\"]:\n",
    "        if c in df.columns:\n",
    "            keep.append(c)\n",
    "    return df[keep]\n",
    "\n",
    "def read_sarea_min(path: Path) -> pd.DataFrame:\n",
    "    keep = {\n",
    "        \"contract_id\",\n",
    "        \"contract_number\",\n",
    "        \"plan_id\",\n",
    "        \"plan\",\n",
    "        \"planid\",\n",
    "        \"plan_number\",\n",
    "        \"fips\",\n",
    "        \"fips_state_county_code\",\n",
    "        \"fipscounty\",\n",
    "        \"fipscnty\",\n",
    "        \"county_fips\",\n",
    "        \"cnty_fips\",\n",
    "    }\n",
    "\n",
    "    df = read_csv_any(path, usecols=lambda c: norm_colname(c) in keep)\n",
    "    df = normalize_columns(df)\n",
    "\n",
    "    cols = list(df.columns)\n",
    "    contract_col = first_existing_col(cols, [\"contract_id\", \"contract_number\", \"contract\"])\n",
    "    if contract_col is None:\n",
    "        raise KeyError(f\"Service area missing contract column. Sample cols: {sorted(cols)[:60]}\")\n",
    "    df[\"contract_id\"] = coerce_id_series(df[contract_col], None)\n",
    "\n",
    "    plan_col = first_existing_col(cols, [\"plan_id\", \"planid\", \"plan\", \"plan_number\"])\n",
    "    if plan_col is None:\n",
    "        df[\"plan_id\"] = np.nan\n",
    "    else:\n",
    "        df[\"plan_id\"] = coerce_id_series(df[plan_col], 3)\n",
    "\n",
    "    df[\"fips\"] = coerce_fips(df)\n",
    "\n",
    "    return df[[\"contract_id\", \"plan_id\", \"fips\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2fd30ed-d802-4a52-b4fd-58c9d678d852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_exclusions(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    df[\"plan_id\"] = df[\"plan_id\"].astype(str).str.strip().str.zfill(3)\n",
    "    df[\"is_800_series\"] = df[\"plan_id\"].str.startswith(\"8\")\n",
    "\n",
    "    cols = list(df.columns)\n",
    "\n",
    "    snp_col = first_existing_col(cols, [\"snp\", \"snp_type\"])\n",
    "    if snp_col is None:\n",
    "        df[\"is_snp\"] = False\n",
    "    else:\n",
    "        s = df[snp_col].astype(str).str.lower().str.strip()\n",
    "        df[\"is_snp\"] = s.isin([\"y\", \"yes\", \"1\", \"true\"]) | (s.notna() & (s != \"\") & ~s.isin([\"n\", \"no\", \"0\", \"false\", \"nan\", \"none\"]))\n",
    "\n",
    "    partc_col = first_existing_col(cols, [\"partc\", \"part_c\"])\n",
    "    if partc_col is not None:\n",
    "        s = df[partc_col].astype(str).str.lower().str.strip()\n",
    "        df[\"is_pdp_only\"] = s.isin([\"n\", \"no\", \"0\", \"false\"])\n",
    "    else:\n",
    "        plan_type_col = first_existing_col(cols, [\"plan_type\", \"contract_type\"])\n",
    "        if plan_type_col is None:\n",
    "            df[\"is_pdp_only\"] = False\n",
    "        else:\n",
    "            s = df[plan_type_col].astype(str).str.lower().str.strip()\n",
    "            df[\"is_pdp_only\"] = s.str.contains(r\"\\bpdp\\b\", regex=True) | s.str.contains(\"prescription drug\", regex=False)\n",
    "\n",
    "    df[\"drop_hw2\"] = df[\"is_snp\"] | df[\"is_800_series\"] | df[\"is_pdp_only\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81815f0d-f2d0-4df8-b0e0-7263f4a12f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_plan_county_year(year: int) -> Path:\n",
    "    out_path = CACHE_DIR / f\"plan_county_year_{year}.csv\"\n",
    "    if out_path.exists():\n",
    "        return out_path\n",
    "\n",
    "    accum = None\n",
    "\n",
    "    for m in range(1, 13):\n",
    "        enroll_path = find_month_file(ENROLL_EXTRACTED, year, m, \"enroll\")\n",
    "        sarea_path  = find_month_file(SAREA_EXTRACTED,  year, m, \"sarea\")\n",
    "\n",
    "        enroll = read_enroll_min(enroll_path)\n",
    "        sarea  = read_sarea_min(sarea_path)\n",
    "\n",
    "        enroll = enroll.dropna(subset=[\"contract_id\", \"plan_id\", \"fips\"])\n",
    "        sarea  = sarea.dropna(subset=[\"contract_id\", \"fips\"])\n",
    "\n",
    "        has_plan = sarea[\"plan_id\"].notna().any()\n",
    "        if has_plan:\n",
    "            sarea = sarea.dropna(subset=[\"plan_id\"])\n",
    "            merged = enroll.merge(sarea, on=[\"contract_id\", \"plan_id\", \"fips\"], how=\"inner\")\n",
    "        else:\n",
    "            merged = enroll.merge(sarea[[\"contract_id\", \"fips\"]], on=[\"contract_id\", \"fips\"], how=\"inner\")\n",
    "\n",
    "        merged[\"year\"] = int(year)\n",
    "\n",
    "        merged = flag_exclusions(merged)\n",
    "\n",
    "        g = merged.groupby([\"contract_id\", \"plan_id\", \"fips\", \"year\"], as_index=False).agg(\n",
    "            enroll_sum=(\"enrollment\", \"sum\"),\n",
    "            is_snp=(\"is_snp\", \"max\"),\n",
    "            is_800_series=(\"is_800_series\", \"max\"),\n",
    "            is_pdp_only=(\"is_pdp_only\", \"max\"),\n",
    "            drop_hw2=(\"drop_hw2\", \"max\"),\n",
    "        )\n",
    "\n",
    "        g[\"months_observed\"] = 1\n",
    "        g[\"dec_enrollment\"] = g[\"enroll_sum\"] if m == 12 else np.nan\n",
    "\n",
    "        if accum is None:\n",
    "            accum = g\n",
    "        else:\n",
    "            keys = [\"contract_id\", \"plan_id\", \"fips\", \"year\"]\n",
    "            accum = accum.merge(g, on=keys, how=\"outer\", suffixes=(\"\", \"_new\"))\n",
    "\n",
    "            accum[\"enroll_sum\"] = accum[\"enroll_sum\"].fillna(0) + accum[\"enroll_sum_new\"].fillna(0)\n",
    "            accum[\"months_observed\"] = accum[\"months_observed\"].fillna(0) + accum[\"months_observed_new\"].fillna(0)\n",
    "\n",
    "            for c in [\"is_snp\", \"is_800_series\", \"is_pdp_only\", \"drop_hw2\"]:\n",
    "                accum[c] = np.maximum(\n",
    "                    pd.to_numeric(accum[c], errors=\"coerce\").fillna(0),\n",
    "                    pd.to_numeric(accum[f\"{c}_new\"], errors=\"coerce\").fillna(0),\n",
    "                ).astype(int)\n",
    "\n",
    "            accum[\"dec_enrollment\"] = accum[\"dec_enrollment\"].combine_first(accum[\"dec_enrollment_new\"])\n",
    "\n",
    "            dropcols = [c for c in accum.columns if c.endswith(\"_new\")]\n",
    "            accum = accum.drop(columns=dropcols)\n",
    "\n",
    "        del enroll, sarea, merged, g\n",
    "        gc.collect()\n",
    "\n",
    "    accum[\"avg_enrollment\"] = accum[\"enroll_sum\"] / accum[\"months_observed\"].replace({0: np.nan})\n",
    "\n",
    "    out = accum[\n",
    "        [\n",
    "            \"contract_id\",\n",
    "            \"plan_id\",\n",
    "            \"fips\",\n",
    "            \"year\",\n",
    "            \"avg_enrollment\",\n",
    "            \"dec_enrollment\",\n",
    "            \"months_observed\",\n",
    "            \"is_snp\",\n",
    "            \"is_800_series\",\n",
    "            \"is_pdp_only\",\n",
    "            \"drop_hw2\",\n",
    "        ]\n",
    "    ].copy()\n",
    "\n",
    "    out[\"contract_id\"] = out[\"contract_id\"].astype(str).str.strip()\n",
    "    out[\"plan_id\"] = out[\"plan_id\"].astype(str).str.strip().str.zfill(3)\n",
    "    out[\"fips\"] = out[\"fips\"].astype(str).str.strip().str.zfill(5)\n",
    "    out[\"year\"] = pd.to_numeric(out[\"year\"], errors=\"coerce\").astype(int)\n",
    "\n",
    "    for c in [\"is_snp\", \"is_800_series\", \"is_pdp_only\", \"drop_hw2\"]:\n",
    "        out[c] = pd.to_numeric(out[c], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "    for c in [\"avg_enrollment\", \"dec_enrollment\", \"months_observed\"]:\n",
    "        out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "\n",
    "    out.to_csv(out_path, index=False)\n",
    "\n",
    "    del accum, out\n",
    "    gc.collect()\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82f4d2cf-628b-4527-afc8-7ae897bd8645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_county_plan_counts_2014_2019() -> Path:\n",
    "    out_path = OUT_DIR / \"county_plan_counts_2014_2019.csv\"\n",
    "    if out_path.exists():\n",
    "        return out_path\n",
    "\n",
    "    counts = {}\n",
    "\n",
    "    for y in YEARS:\n",
    "        print(\"Counting plans. Year:\", y)\n",
    "        yp = build_plan_county_year(y)\n",
    "\n",
    "        usecols = [\"fips\", \"year\", \"contract_id\", \"plan_id\", \"drop_hw2\"]\n",
    "        for chunk in pd.read_csv(yp, dtype=str, usecols=usecols, chunksize=CHUNKSIZE):\n",
    "            drop = chunk[\"drop_hw2\"].astype(str).str.lower().str.strip().isin([\"1\", \"true\", \"t\", \"yes\", \"y\"])\n",
    "            chunk = chunk.loc[~drop, [\"fips\", \"year\", \"contract_id\", \"plan_id\"]].dropna()\n",
    "\n",
    "            chunk[\"plan_id\"] = chunk[\"plan_id\"].astype(str).str.zfill(3)\n",
    "            chunk[\"plan_key\"] = chunk[\"contract_id\"].astype(str).str.strip() + \"-\" + chunk[\"plan_id\"].astype(str).str.strip()\n",
    "\n",
    "            chunk = chunk.drop_duplicates(subset=[\"fips\", \"year\", \"plan_key\"])\n",
    "\n",
    "            for (fips, year), g in chunk.groupby([\"fips\", \"year\"]):\n",
    "                counts[(fips, year)] = counts.get((fips, year), 0) + int(g[\"plan_key\"].nunique())\n",
    "\n",
    "            del chunk\n",
    "            gc.collect()\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    out = pd.DataFrame([(k[0], k[1], v) for k, v in counts.items()], columns=[\"fips\", \"year\", \"plan_count\"])\n",
    "    out = out.sort_values([\"year\", \"fips\"]).reset_index(drop=True)\n",
    "    out.to_csv(out_path, index=False)\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd928307-d388-4d6c-aad8-6a83ecfebf47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penetration shape: (19494, 4)\n",
      "    fips  ma_enrollment  eligibles  year\n",
      "0  01001         3000.0       9438  2014\n",
      "1  01003        13183.0      41640  2014\n",
      "2  01005          859.0       6004  2014\n",
      "3  01007         1665.0       4599  2014\n",
      "4  01009         4680.0      11193  2014\n"
     ]
    }
   ],
   "source": [
    "def read_penetration_year(year: int) -> pd.DataFrame:\n",
    "    path = find_year_file(PEN_EXTRACTED, year, \"penetration\")\n",
    "    df = read_csv_any(path)\n",
    "    df = normalize_columns(df)\n",
    "\n",
    "    df[\"fips\"] = coerce_fips(df)\n",
    "\n",
    "    cols = list(df.columns)\n",
    "    ma_col = first_existing_col(cols, [\"ma_enrollment\", \"ma_enroll\", \"enrolled\", \"enroll\"])\n",
    "    elig_col = first_existing_col(cols, [\"eligibles\", \"total_eligibles\", \"medicare_eligibles\", \"tot_eligibles\"])\n",
    "\n",
    "    if ma_col is None or elig_col is None:\n",
    "        raise KeyError(f\"Penetration file missing enrolled or eligibles. Col sample: {sorted(cols)[:100]}\")\n",
    "\n",
    "    out = df[[\"fips\", ma_col, elig_col]].copy()\n",
    "    out = out.rename(columns={ma_col: \"ma_enrollment\", elig_col: \"eligibles\"})\n",
    "    out[\"ma_enrollment\"] = to_numeric_clean(out[\"ma_enrollment\"])\n",
    "    out[\"eligibles\"] = to_numeric_clean(out[\"eligibles\"])\n",
    "    out[\"year\"] = int(year)\n",
    "    out = out.dropna(subset=[\"fips\"])\n",
    "    out[\"fips\"] = out[\"fips\"].astype(str).str.zfill(5)\n",
    "    return out\n",
    "\n",
    "pen = pd.concat([read_penetration_year(y) for y in YEARS], ignore_index=True)\n",
    "print(\"Penetration shape:\", pen.shape)\n",
    "print(pen.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "693017f3-e4f7-4ca8-a20d-0c38c3357b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_county_hhi_ma_share_2014_2019(pen: pd.DataFrame) -> Path:\n",
    "    out_path = OUT_DIR / \"county_hhi_ma_share_2014_2019.csv\"\n",
    "    if out_path.exists():\n",
    "        return out_path\n",
    "\n",
    "    pen2 = pen.copy()\n",
    "    pen2[\"year\"] = pen2[\"year\"].astype(int)\n",
    "    pen2[\"fips\"] = pen2[\"fips\"].astype(str).str.zfill(5)\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for y in YEARS:\n",
    "        print(\"HHI and MA share. Year:\", y)\n",
    "        yp = build_plan_county_year(y)\n",
    "\n",
    "        usecols = [\"fips\", \"year\", \"contract_id\", \"plan_id\", \"dec_enrollment\", \"avg_enrollment\", \"drop_hw2\"]\n",
    "        pieces = []\n",
    "\n",
    "        for chunk in pd.read_csv(yp, dtype=str, usecols=usecols, chunksize=CHUNKSIZE):\n",
    "            drop = chunk[\"drop_hw2\"].astype(str).str.lower().str.strip().isin([\"1\", \"true\", \"t\", \"yes\", \"y\"])\n",
    "            chunk = chunk.loc[~drop].copy()\n",
    "\n",
    "            dec = pd.to_numeric(chunk[\"dec_enrollment\"], errors=\"coerce\")\n",
    "            avg = pd.to_numeric(chunk[\"avg_enrollment\"], errors=\"coerce\")\n",
    "            e = dec.fillna(avg).fillna(0.0)\n",
    "\n",
    "            chunk[\"e\"] = e\n",
    "            chunk[\"e2\"] = e * e\n",
    "\n",
    "            tmp = (\n",
    "                chunk.groupby([\"fips\", \"year\"], as_index=False)\n",
    "                     .agg(total_enroll=(\"e\", \"sum\"), sumsq_enroll=(\"e2\", \"sum\"))\n",
    "            )\n",
    "            pieces.append(tmp)\n",
    "\n",
    "            del chunk, tmp\n",
    "            gc.collect()\n",
    "\n",
    "        out = pd.concat(pieces, ignore_index=True)\n",
    "        out = out.groupby([\"fips\", \"year\"], as_index=False).agg(\n",
    "            total_enroll=(\"total_enroll\", \"sum\"),\n",
    "            sumsq_enroll=(\"sumsq_enroll\", \"sum\"),\n",
    "        )\n",
    "\n",
    "        out[\"hhi\"] = out[\"sumsq_enroll\"] / (out[\"total_enroll\"] * out[\"total_enroll\"])\n",
    "        out = out.rename(columns={\"total_enroll\": \"ma_total_from_plans\"})\n",
    "        out[\"fips\"] = out[\"fips\"].astype(str).str.zfill(5)\n",
    "        out[\"year\"] = out[\"year\"].astype(int)\n",
    "\n",
    "        out = out.merge(pen2, on=[\"fips\", \"year\"], how=\"left\")\n",
    "        out[\"ma_share\"] = out[\"ma_enrollment\"] / out[\"eligibles\"]\n",
    "\n",
    "        rows.append(out)\n",
    "\n",
    "        del out, pieces\n",
    "        gc.collect()\n",
    "\n",
    "    final = pd.concat(rows, ignore_index=True).sort_values([\"year\", \"fips\"]).reset_index(drop=True)\n",
    "    final.to_csv(out_path, index=False)\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4990c95-5854-47ac-9ce1-7b08c38d9f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: data/processed/county_plan_counts_2014_2019.csv\n",
      "    fips  year plan_count\n",
      "0  01001  2014        386\n",
      "1  01003  2014        431\n",
      "2  01005  2014        370\n",
      "3  01007  2014        384\n",
      "4  01009  2014        387\n",
      "Wrote: data/processed/county_hhi_ma_share_2014_2019.csv\n",
      "    fips  year ma_total_from_plans sumsq_enroll                  hhi ma_enrollment eligibles             ma_share\n",
      "0  01001  2014              3027.0    1223071.0  0.13348326683022055        3000.0    9438.0   0.3178639542275906\n",
      "1  01003  2014             13627.0   27941489.0  0.15046969271875732       13183.0   41640.0   0.3165946205571566\n",
      "2  01005  2014               863.0     252081.0   0.3384687064042676         859.0    6004.0  0.14307128580946035\n",
      "3  01007  2014              1661.0     484639.0  0.17566251443952183        1665.0    4599.0  0.36203522504892366\n",
      "4  01009  2014              4653.0    3412299.0  0.15760898558544553        4680.0   11193.0   0.4181184668989547\n"
     ]
    }
   ],
   "source": [
    "county_counts_path = build_county_plan_counts_2014_2019()\n",
    "county_hhi_path = build_county_hhi_ma_share_2014_2019(pen)\n",
    "\n",
    "print(\"Wrote:\", county_counts_path)\n",
    "print(pd.read_csv(county_counts_path, dtype=str, nrows=5))\n",
    "\n",
    "print(\"Wrote:\", county_hhi_path)\n",
    "print(pd.read_csv(county_hhi_path, dtype=str, nrows=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd68f598-5762-4d55-b6f9-33354b799bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated .gitignore\n",
      "Keep these for HW2 analysis:\n",
      "  data/processed/county_plan_counts_2014_2019.csv\n",
      "  data/processed/county_hhi_ma_share_2014_2019.csv\n",
      "Do not push data/cache\n"
     ]
    }
   ],
   "source": [
    "gitignore_path = Path(\".gitignore\")\n",
    "existing = gitignore_path.read_text().splitlines() if gitignore_path.exists() else []\n",
    "\n",
    "rules = [\n",
    "    \"data/cache/\",\n",
    "]\n",
    "\n",
    "new_lines = existing[:]\n",
    "for r in rules:\n",
    "    if r not in new_lines:\n",
    "        new_lines.append(r)\n",
    "\n",
    "gitignore_path.write_text(\"\\n\".join(new_lines) + \"\\n\")\n",
    "\n",
    "print(\"Updated .gitignore\")\n",
    "print(\"Keep these for HW2 analysis:\")\n",
    "print(\" \", OUT_DIR / \"county_plan_counts_2014_2019.csv\")\n",
    "print(\" \", OUT_DIR / \"county_hhi_ma_share_2014_2019.csv\")\n",
    "print(\"Do not push data/cache\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
