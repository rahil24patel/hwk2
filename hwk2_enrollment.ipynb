{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49c4402b-7819-4773-a21e-e492b8ddc76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MA_ROOT: /scion/5261/econ470001/ma-data/ma\n",
      "ENROLL_EXTRACTED exists: True\n",
      "SAREA_EXTRACTED exists: True\n",
      "PEN_EXTRACTED exists: True\n",
      "FORCE_REBUILD_CACHE: True\n",
      "FORCE_REBUILD_OUTPUTS: True\n",
      "Deleted 6 cache files in data/cache/\n",
      "Deleted output: data/processed/county_plan_counts_2014_2019.csv\n",
      "Deleted output: data/processed/county_hhi_ma_share_2014_2019.csv\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "YEARS = list(range(2014, 2020))\n",
    "\n",
    "CACHE_DIR = Path(\"data/cache\")\n",
    "OUT_DIR   = Path(\"data/processed\")\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHUNKSIZE = 300_000\n",
    "\n",
    "CANDIDATE_MA_ROOTS = [\n",
    "    Path(\"/scion/5261/econ470001/ma-data/ma\"),\n",
    "    Path(\"/econ470/a0/work/ma-data/ma\"),\n",
    "    Path.cwd().parent / \"ma-data\" / \"ma\",\n",
    "]\n",
    "\n",
    "def pick_existing(paths: list[Path]) -> Path | None:\n",
    "    for p in paths:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "MA_ROOT = pick_existing(CANDIDATE_MA_ROOTS)\n",
    "if MA_ROOT is None:\n",
    "    raise FileNotFoundError(\"Could not find ma-data/ma. Update CANDIDATE_MA_ROOTS if needed.\")\n",
    "\n",
    "ENROLL_EXTRACTED = MA_ROOT / \"enrollment\" / \"Extracted Data\"\n",
    "SAREA_EXTRACTED  = MA_ROOT / \"service-area\" / \"Extracted Data\"\n",
    "PEN_EXTRACTED    = MA_ROOT / \"penetration\" / \"Extracted Data\"\n",
    "\n",
    "print(\"MA_ROOT:\", MA_ROOT)\n",
    "print(\"ENROLL_EXTRACTED exists:\", ENROLL_EXTRACTED.exists())\n",
    "print(\"SAREA_EXTRACTED exists:\", SAREA_EXTRACTED.exists())\n",
    "print(\"PEN_EXTRACTED exists:\", PEN_EXTRACTED.exists())\n",
    "\n",
    "FORCE_REBUILD_CACHE   = True\n",
    "FORCE_REBUILD_OUTPUTS = True\n",
    "\n",
    "print(\"FORCE_REBUILD_CACHE:\", FORCE_REBUILD_CACHE)\n",
    "print(\"FORCE_REBUILD_OUTPUTS:\", FORCE_REBUILD_OUTPUTS)\n",
    "\n",
    "# Optional cleanup\n",
    "if FORCE_REBUILD_CACHE:\n",
    "    n = 0\n",
    "    for p in CACHE_DIR.glob(\"plan_county_year_*.csv\"):\n",
    "        p.unlink()\n",
    "        n += 1\n",
    "    print(f\"Deleted {n} cache files in {CACHE_DIR}/\")\n",
    "\n",
    "if FORCE_REBUILD_OUTPUTS:\n",
    "    targets = [\n",
    "        OUT_DIR / \"county_plan_counts_2014_2019.csv\",\n",
    "        OUT_DIR / \"county_hhi_ma_share_2014_2019.csv\",\n",
    "    ]\n",
    "    for p in targets:\n",
    "        if p.exists():\n",
    "            p.unlink()\n",
    "            print(\"Deleted output:\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e405368b-1422-4863-b7bd-8b4443ad0584",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRUE_SET  = {\"1\", \"1.0\", \"true\", \"t\", \"yes\", \"y\"}\n",
    "FALSE_SET = {\"0\", \"0.0\", \"false\", \"f\", \"no\", \"n\", \"nan\", \"none\", \"\"}\n",
    "\n",
    "def parse_boolish_series(x: pd.Series) -> pd.Series:\n",
    "    s = x.astype(str).str.strip().str.lower()\n",
    "    s = s.replace({\"<na>\": \"\", \"na\": \"\", \"n/a\": \"\"})\n",
    "\n",
    "    out = pd.Series(False, index=s.index)\n",
    "    out[s.isin(TRUE_SET)] = True\n",
    "    out[s.isin(FALSE_SET)] = False\n",
    "\n",
    "    num = pd.to_numeric(s, errors=\"coerce\")\n",
    "    out[num.notna()] = (num[num.notna()] != 0)\n",
    "    return out\n",
    "\n",
    "def norm_colname(s: str) -> str:\n",
    "    s = str(s).strip().lower()\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-z0-9_]\", \"\", s)\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s\n",
    "\n",
    "def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [norm_colname(c) for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def first_existing_col(cols: list[str], candidates: list[str]) -> str | None:\n",
    "    s = set(cols)\n",
    "    for c in candidates:\n",
    "        if c in s:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def coerce_id_series(x: pd.Series, width: int | None = None) -> pd.Series:\n",
    "    s = x.astype(str).str.strip()\n",
    "    s = s.replace({\"nan\": np.nan, \"None\": np.nan, \"\": np.nan})\n",
    "    s = s.str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "    if width is not None:\n",
    "        s = s.str.zfill(width)\n",
    "    return s\n",
    "\n",
    "def to_numeric_clean(x: pd.Series) -> pd.Series:\n",
    "    s = x.astype(str).str.replace(\",\", \"\", regex=False).str.strip()\n",
    "    s = s.replace({\"nan\": np.nan, \"None\": np.nan, \"\": np.nan, \"*\": np.nan, \"$-\": np.nan, \"-\": np.nan})\n",
    "    s = s.str.replace(\"$\", \"\", regex=False)\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "def _digits_zfill(x: pd.Series, width: int) -> pd.Series:\n",
    "    s = x.astype(str).str.strip()\n",
    "    s = s.replace({\"nan\": np.nan, \"None\": np.nan, \"\": np.nan})\n",
    "    s = s.str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "    s = s.str.replace(r\"[^0-9]\", \"\", regex=True)\n",
    "    s = s.where(s.str.len() > 0, np.nan)\n",
    "    return s.str.zfill(width)\n",
    "\n",
    "def coerce_fips(df: pd.DataFrame) -> pd.Series:\n",
    "    cols = list(df.columns)\n",
    "\n",
    "    fips_col = first_existing_col(\n",
    "        cols,\n",
    "        [\n",
    "            \"fips\",\n",
    "            \"fips_state_county_code\",\n",
    "            \"fipsstatecountycode\",\n",
    "            \"county_fips\",\n",
    "            \"countyfips\",\n",
    "            \"cnty_fips\",\n",
    "            \"fipscounty\",\n",
    "            \"fipscnty\",\n",
    "        ],\n",
    "    )\n",
    "    if fips_col is not None:\n",
    "        return _digits_zfill(df[fips_col], 5)\n",
    "\n",
    "    st_col = first_existing_col(cols, [\"state_fips\", \"statefips\", \"fipsst\", \"fips_state\"])\n",
    "    ct_col = first_existing_col(cols, [\"county_fips\", \"countyfips\", \"fipscnty\", \"fips_county\"])\n",
    "\n",
    "    if st_col is not None and ct_col is not None:\n",
    "        st = _digits_zfill(df[st_col], 2)\n",
    "        ct = _digits_zfill(df[ct_col], 3)\n",
    "        out = (st.fillna(\"\") + ct.fillna(\"\")).replace({\"\": np.nan})\n",
    "        return out\n",
    "\n",
    "    return pd.Series([np.nan] * len(df), index=df.index, dtype=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56c5f0b0-8622-4720-96d3-68caf2687208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_month_file(root: Path, year: int, month: int, kind: str) -> Path:\n",
    "    m2 = f\"{month:02d}\"\n",
    "    files = [p for p in root.rglob(\"*\") if p.is_file()]\n",
    "\n",
    "    if kind == \"enroll\":\n",
    "        pats = [rf\".*{year}.*{m2}.*enroll.*\\.(csv|txt)$\", rf\".*enroll.*{year}.*{m2}.*\\.(csv|txt)$\"]\n",
    "    elif kind == \"sarea\":\n",
    "        pats = [\n",
    "            rf\".*{year}.*{m2}.*(service|sa|sarea|cnty_sa|service_area).*?\\.(csv|txt)$\",\n",
    "            rf\".*(service|sa|sarea|cnty_sa|service_area).*{year}.*{m2}.*?\\.(csv|txt)$\",\n",
    "        ]\n",
    "    else:\n",
    "        raise ValueError(\"unknown kind\")\n",
    "\n",
    "    for pat in pats:\n",
    "        rx = re.compile(pat, flags=re.I)\n",
    "        hits = [p for p in files if rx.match(p.name)]\n",
    "        if hits:\n",
    "            hits.sort(key=lambda p: len(p.name))\n",
    "            return hits[0]\n",
    "\n",
    "    rx = re.compile(rf\".*{year}.*{m2}.*\\.(csv|txt)$\", flags=re.I)\n",
    "    hits = [p for p in files if rx.match(p.name)]\n",
    "    if hits:\n",
    "        hits.sort(key=lambda p: len(p.name))\n",
    "        return hits[0]\n",
    "\n",
    "    raise FileNotFoundError(f\"Could not find {kind} file for {year}-{m2} under {root}\")\n",
    "\n",
    "def find_year_file(root: Path, year: int, kind: str) -> Path:\n",
    "    files = [p for p in root.rglob(\"*\") if p.is_file()]\n",
    "    if kind != \"penetration\":\n",
    "        raise ValueError(\"unknown kind\")\n",
    "\n",
    "    pats = [rf\".*{year}.*penet.*\\.(csv|txt)$\", rf\".*penet.*{year}.*\\.(csv|txt)$\"]\n",
    "    for pat in pats:\n",
    "        rx = re.compile(pat, flags=re.I)\n",
    "        hits = [p for p in files if rx.match(p.name)]\n",
    "        if hits:\n",
    "            hits.sort(key=lambda p: len(p.name))\n",
    "            return hits[0]\n",
    "\n",
    "    rx = re.compile(rf\".*{year}.*\\.(csv|txt)$\", flags=re.I)\n",
    "    hits = [p for p in files if rx.match(p.name)]\n",
    "    if hits:\n",
    "        hits.sort(key=lambda p: len(p.name))\n",
    "        return hits[0]\n",
    "\n",
    "    raise FileNotFoundError(f\"Could not find {kind} file for {year} under {root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "681e1681-858b-4c6a-b87d-b5215129e0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_any(path: Path, usecols=None) -> pd.DataFrame:\n",
    "    return pd.read_csv(\n",
    "        path,\n",
    "        dtype=str,\n",
    "        na_values=[\"*\", \"NA\", \"N/A\", \"\"],\n",
    "        keep_default_na=True,\n",
    "        encoding_errors=\"replace\",\n",
    "        low_memory=False,\n",
    "        usecols=usecols,\n",
    "    )\n",
    "\n",
    "def read_enroll_min(path: Path) -> pd.DataFrame:\n",
    "    want = {\n",
    "        \"contract_number\",\n",
    "        \"contract_id\",\n",
    "        \"plan_id\",\n",
    "        \"fips_state_county_code\",\n",
    "        \"enrollment\",\n",
    "        \"snp\",\n",
    "        \"snp_type\",\n",
    "        \"partc\",\n",
    "        \"part_c\",\n",
    "        \"plan_type\",\n",
    "        \"contract_type\",\n",
    "    }\n",
    "\n",
    "    df = read_csv_any(path, usecols=lambda c: norm_colname(c) in want)\n",
    "    df = normalize_columns(df)\n",
    "\n",
    "    contract_col = first_existing_col(list(df.columns), [\"contract_id\", \"contract_number\"])\n",
    "    if contract_col is None:\n",
    "        raise KeyError(\"Enrollment file missing contract column\")\n",
    "    if \"plan_id\" not in df.columns:\n",
    "        raise KeyError(\"Enrollment file missing plan_id column\")\n",
    "    if \"enrollment\" not in df.columns:\n",
    "        raise KeyError(\"Enrollment file missing enrollment column\")\n",
    "\n",
    "    df[\"contract_id\"] = coerce_id_series(df[contract_col], None)\n",
    "    df[\"plan_id\"] = coerce_id_series(df[\"plan_id\"], 3)\n",
    "\n",
    "    fips_src = df.get(\"fips_state_county_code\", pd.Series([np.nan] * len(df), index=df.index))\n",
    "    df[\"fips\"] = _digits_zfill(fips_src, 5)\n",
    "\n",
    "    df[\"enrollment\"] = to_numeric_clean(df[\"enrollment\"])\n",
    "\n",
    "    keep = [\"contract_id\", \"plan_id\", \"fips\", \"enrollment\"]\n",
    "    for c in [\"snp\", \"snp_type\", \"partc\", \"part_c\", \"plan_type\", \"contract_type\"]:\n",
    "        if c in df.columns:\n",
    "            keep.append(c)\n",
    "\n",
    "    return df[keep]\n",
    "\n",
    "def read_sarea_min(path: Path) -> pd.DataFrame:\n",
    "    keep = {\n",
    "        \"contract_id\",\n",
    "        \"contract_number\",\n",
    "        \"plan_id\",\n",
    "        \"plan\",\n",
    "        \"planid\",\n",
    "        \"plan_number\",\n",
    "        \"fips\",\n",
    "        \"fips_state_county_code\",\n",
    "        \"fipscounty\",\n",
    "        \"fipscnty\",\n",
    "        \"county_fips\",\n",
    "        \"cnty_fips\",\n",
    "    }\n",
    "\n",
    "    df = read_csv_any(path, usecols=lambda c: norm_colname(c) in keep)\n",
    "    df = normalize_columns(df)\n",
    "\n",
    "    cols = list(df.columns)\n",
    "    contract_col = first_existing_col(cols, [\"contract_id\", \"contract_number\", \"contract\"])\n",
    "    if contract_col is None:\n",
    "        raise KeyError(f\"Service area missing contract column. Sample cols: {sorted(cols)[:60]}\")\n",
    "\n",
    "    df[\"contract_id\"] = coerce_id_series(df[contract_col], None)\n",
    "\n",
    "    plan_col = first_existing_col(cols, [\"plan_id\", \"planid\", \"plan\", \"plan_number\"])\n",
    "    if plan_col is None:\n",
    "        df[\"plan_id\"] = np.nan\n",
    "    else:\n",
    "        df[\"plan_id\"] = coerce_id_series(df[plan_col], 3)\n",
    "\n",
    "    df[\"fips\"] = coerce_fips(df)\n",
    "\n",
    "    return df[[\"contract_id\", \"plan_id\", \"fips\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c7e971f-474a-4adc-986a-447d6232e0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_exclusions(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Produces:\n",
    "      is_snp, is_800_series, is_pdp_only, drop_hw2  (all 0/1 ints)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    df[\"plan_id\"] = df[\"plan_id\"].astype(str).str.strip().str.zfill(3)\n",
    "    df[\"is_800_series\"] = df[\"plan_id\"].str.startswith(\"8\").astype(int)\n",
    "\n",
    "    cols = list(df.columns)\n",
    "\n",
    "    # SNP\n",
    "    snp_col = first_existing_col(cols, [\"snp\"])\n",
    "    snp_type_col = first_existing_col(cols, [\"snp_type\"])\n",
    "\n",
    "    if snp_col is not None:\n",
    "        df[\"is_snp\"] = parse_boolish_series(df[snp_col]).astype(int)\n",
    "    elif snp_type_col is not None:\n",
    "        st = df[snp_type_col].astype(str).str.strip().str.lower()\n",
    "        df[\"is_snp\"] = st.str.contains(\"snp\", na=False).astype(int)\n",
    "    else:\n",
    "        df[\"is_snp\"] = 0\n",
    "\n",
    "    # PDP-only (no Part C)\n",
    "    partc_col = first_existing_col(cols, [\"partc\", \"part_c\"])\n",
    "    if partc_col is not None:\n",
    "        is_partc = parse_boolish_series(df[partc_col])\n",
    "        df[\"is_pdp_only\"] = (~is_partc).astype(int)\n",
    "    else:\n",
    "        plan_type_col = first_existing_col(cols, [\"plan_type\", \"contract_type\"])\n",
    "        if plan_type_col is None:\n",
    "            df[\"is_pdp_only\"] = 0\n",
    "        else:\n",
    "            s = df[plan_type_col].astype(str).str.lower().str.strip()\n",
    "            df[\"is_pdp_only\"] = (\n",
    "                s.str.contains(r\"\\bpdp\\b\", regex=True, na=False)\n",
    "                | s.str.contains(\"prescription drug\", na=False)\n",
    "                | s.str.contains(\"part d\", na=False)\n",
    "            ).astype(int)\n",
    "\n",
    "    df[\"drop_hw2\"] = ((df[\"is_snp\"] == 1) | (df[\"is_800_series\"] == 1) | (df[\"is_pdp_only\"] == 1)).astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f586e740-2bde-497c-9711-02319da47118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_plan_county_year(year: int) -> Path:\n",
    "    out_path = CACHE_DIR / f\"plan_county_year_{year}.csv\"\n",
    "\n",
    "    if FORCE_REBUILD_CACHE and out_path.exists():\n",
    "        out_path.unlink()\n",
    "        print(\"Deleted cache:\", out_path)\n",
    "\n",
    "    if out_path.exists():\n",
    "        return out_path\n",
    "\n",
    "    accum = None\n",
    "\n",
    "    for m in range(1, 13):\n",
    "        enroll_path = find_month_file(ENROLL_EXTRACTED, year, m, \"enroll\")\n",
    "        sarea_path  = find_month_file(SAREA_EXTRACTED,  year, m, \"sarea\")\n",
    "\n",
    "        enroll = read_enroll_min(enroll_path)\n",
    "        sarea  = read_sarea_min(sarea_path)\n",
    "\n",
    "        enroll = enroll.dropna(subset=[\"contract_id\", \"plan_id\", \"fips\"])\n",
    "        sarea  = sarea.dropna(subset=[\"contract_id\", \"fips\"])\n",
    "\n",
    "        has_plan = sarea[\"plan_id\"].notna().any()\n",
    "        if has_plan:\n",
    "            sarea = sarea.dropna(subset=[\"plan_id\"])\n",
    "            merged = enroll.merge(sarea, on=[\"contract_id\", \"plan_id\", \"fips\"], how=\"inner\")\n",
    "        else:\n",
    "            merged = enroll.merge(sarea[[\"contract_id\", \"fips\"]], on=[\"contract_id\", \"fips\"], how=\"inner\")\n",
    "\n",
    "        merged[\"year\"] = int(year)\n",
    "        merged = flag_exclusions(merged)\n",
    "\n",
    "        g = merged.groupby([\"contract_id\", \"plan_id\", \"fips\", \"year\"], as_index=False).agg(\n",
    "            enroll_sum=(\"enrollment\", \"sum\"),\n",
    "            is_snp=(\"is_snp\", \"max\"),\n",
    "            is_800_series=(\"is_800_series\", \"max\"),\n",
    "            is_pdp_only=(\"is_pdp_only\", \"max\"),\n",
    "            drop_hw2=(\"drop_hw2\", \"max\"),\n",
    "        )\n",
    "\n",
    "        g[\"months_observed\"] = 1\n",
    "        g[\"dec_enrollment\"] = g[\"enroll_sum\"] if m == 12 else np.nan\n",
    "\n",
    "        if accum is None:\n",
    "            accum = g\n",
    "        else:\n",
    "            keys = [\"contract_id\", \"plan_id\", \"fips\", \"year\"]\n",
    "            accum = accum.merge(g, on=keys, how=\"outer\", suffixes=(\"\", \"_new\"))\n",
    "\n",
    "            accum[\"enroll_sum\"] = accum[\"enroll_sum\"].fillna(0) + accum[\"enroll_sum_new\"].fillna(0)\n",
    "            accum[\"months_observed\"] = accum[\"months_observed\"].fillna(0) + accum[\"months_observed_new\"].fillna(0)\n",
    "\n",
    "            for c in [\"is_snp\", \"is_800_series\", \"is_pdp_only\", \"drop_hw2\"]:\n",
    "                accum[c] = np.maximum(\n",
    "                    pd.to_numeric(accum[c], errors=\"coerce\").fillna(0),\n",
    "                    pd.to_numeric(accum[f\"{c}_new\"], errors=\"coerce\").fillna(0),\n",
    "                ).astype(int)\n",
    "\n",
    "            accum[\"dec_enrollment\"] = accum[\"dec_enrollment\"].combine_first(accum[\"dec_enrollment_new\"])\n",
    "\n",
    "            dropcols = [c for c in accum.columns if c.endswith(\"_new\")]\n",
    "            accum = accum.drop(columns=dropcols)\n",
    "\n",
    "        del enroll, sarea, merged, g\n",
    "        gc.collect()\n",
    "\n",
    "    accum[\"avg_enrollment\"] = accum[\"enroll_sum\"] / accum[\"months_observed\"].replace({0: np.nan})\n",
    "\n",
    "    out = accum[\n",
    "        [\n",
    "            \"contract_id\",\n",
    "            \"plan_id\",\n",
    "            \"fips\",\n",
    "            \"year\",\n",
    "            \"avg_enrollment\",\n",
    "            \"dec_enrollment\",\n",
    "            \"months_observed\",\n",
    "            \"is_snp\",\n",
    "            \"is_800_series\",\n",
    "            \"is_pdp_only\",\n",
    "            \"drop_hw2\",\n",
    "        ]\n",
    "    ].copy()\n",
    "\n",
    "    # enforce clean types for downstream notebooks\n",
    "    out[\"contract_id\"] = out[\"contract_id\"].astype(str).str.strip()\n",
    "    out[\"plan_id\"] = out[\"plan_id\"].astype(str).str.strip().str.zfill(3)\n",
    "    out[\"fips\"] = out[\"fips\"].astype(str).str.strip().str.zfill(5)\n",
    "    out[\"year\"] = pd.to_numeric(out[\"year\"], errors=\"coerce\").astype(int)\n",
    "\n",
    "    for c in [\"is_snp\", \"is_800_series\", \"is_pdp_only\", \"drop_hw2\"]:\n",
    "        out[c] = pd.to_numeric(out[c], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "    for c in [\"avg_enrollment\", \"dec_enrollment\", \"months_observed\"]:\n",
    "        out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "\n",
    "    # IMPORTANT: enforce uniqueness\n",
    "    out = out.drop_duplicates(subset=[\"contract_id\", \"plan_id\", \"fips\", \"year\"]).copy()\n",
    "\n",
    "    out.to_csv(out_path, index=False)\n",
    "\n",
    "    del accum, out\n",
    "    gc.collect()\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61ae87ba-db85-49f8-af79-3af28e9c1dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plan counts. Year: 2014\n",
      "Plan counts. Year: 2015\n",
      "Plan counts. Year: 2016\n",
      "Plan counts. Year: 2017\n",
      "Plan counts. Year: 2018\n",
      "Plan counts. Year: 2019\n",
      "Wrote: data/processed/county_plan_counts_2014_2019.csv rows: 19059\n"
     ]
    }
   ],
   "source": [
    "def build_county_plan_counts_2014_2019() -> Path:\n",
    "    out_path = OUT_DIR / \"county_plan_counts_2014_2019.csv\"\n",
    "\n",
    "    if FORCE_REBUILD_OUTPUTS and out_path.exists():\n",
    "        out_path.unlink()\n",
    "        print(\"Deleted output:\", out_path)\n",
    "\n",
    "    if out_path.exists():\n",
    "        return out_path\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for y in YEARS:\n",
    "        print(\"Plan counts. Year:\", y)\n",
    "        yp = build_plan_county_year(y)\n",
    "\n",
    "        usecols = [\"fips\", \"year\", \"drop_hw2\"]\n",
    "        pieces = []\n",
    "        for chunk in pd.read_csv(yp, dtype=str, usecols=usecols, chunksize=CHUNKSIZE):\n",
    "            keep = chunk[\"drop_hw2\"].astype(str).str.strip().isin([\"0\", \"0.0\"])\n",
    "            c = chunk.loc[keep, [\"fips\", \"year\"]].dropna().copy()\n",
    "            c[\"fips\"] = c[\"fips\"].astype(str).str.zfill(5)\n",
    "            c[\"year\"] = pd.to_numeric(c[\"year\"], errors=\"coerce\").astype(int)\n",
    "            # Each row is already one (contract-plan)-county-year after the cache build.\n",
    "            tmp = c.groupby([\"fips\", \"year\"], as_index=False).size().rename(columns={\"size\": \"plan_count\"})\n",
    "            pieces.append(tmp)\n",
    "\n",
    "            del chunk, c, tmp\n",
    "            gc.collect()\n",
    "\n",
    "        out_y = pd.concat(pieces, ignore_index=True).groupby([\"fips\",\"year\"], as_index=False)[\"plan_count\"].sum()\n",
    "        rows.append(out_y)\n",
    "        del pieces, out_y\n",
    "        gc.collect()\n",
    "\n",
    "    out = pd.concat(rows, ignore_index=True).sort_values([\"year\",\"fips\"]).reset_index(drop=True)\n",
    "    out.to_csv(out_path, index=False)\n",
    "    print(\"Wrote:\", out_path, \"rows:\", len(out))\n",
    "    return out_path\n",
    "\n",
    "county_counts_path = build_county_plan_counts_2014_2019()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "118b5325-8fd5-4944-b0d7-9c3702401a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penetration shape: (19494, 4)\n"
     ]
    }
   ],
   "source": [
    "def read_penetration_year(year: int) -> pd.DataFrame:\n",
    "    path = find_year_file(PEN_EXTRACTED, year, \"penetration\")\n",
    "    df = read_csv_any(path)\n",
    "    df = normalize_columns(df)\n",
    "\n",
    "    df[\"fips\"] = coerce_fips(df)\n",
    "\n",
    "    cols = list(df.columns)\n",
    "    ma_col = first_existing_col(cols, [\"ma_enrollment\", \"ma_enroll\", \"enrolled\", \"enroll\"])\n",
    "    elig_col = first_existing_col(cols, [\"eligibles\", \"total_eligibles\", \"medicare_eligibles\", \"tot_eligibles\"])\n",
    "\n",
    "    if ma_col is None or elig_col is None:\n",
    "        raise KeyError(f\"Penetration file missing enrolled or eligibles. Col sample: {sorted(cols)[:100]}\")\n",
    "\n",
    "    out = df[[\"fips\", ma_col, elig_col]].copy()\n",
    "    out = out.rename(columns={ma_col: \"ma_enrollment\", elig_col: \"eligibles\"})\n",
    "    out[\"ma_enrollment\"] = to_numeric_clean(out[\"ma_enrollment\"])\n",
    "    out[\"eligibles\"] = to_numeric_clean(out[\"eligibles\"])\n",
    "    out[\"year\"] = int(year)\n",
    "\n",
    "    out = out.dropna(subset=[\"fips\"])\n",
    "    out[\"fips\"] = out[\"fips\"].astype(str).str.zfill(5)\n",
    "    return out\n",
    "\n",
    "pen = pd.concat([read_penetration_year(y) for y in YEARS], ignore_index=True)\n",
    "print(\"Penetration shape:\", pen.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "129b7d99-a705-4f0b-9e24-4bd7936f1441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HHI + MA share. Year: 2014\n",
      "Deleted cache: data/cache/plan_county_year_2014.csv\n",
      "HHI + MA share. Year: 2015\n",
      "Deleted cache: data/cache/plan_county_year_2015.csv\n",
      "HHI + MA share. Year: 2016\n",
      "Deleted cache: data/cache/plan_county_year_2016.csv\n",
      "HHI + MA share. Year: 2017\n",
      "Deleted cache: data/cache/plan_county_year_2017.csv\n",
      "HHI + MA share. Year: 2018\n",
      "Deleted cache: data/cache/plan_county_year_2018.csv\n",
      "HHI + MA share. Year: 2019\n",
      "Deleted cache: data/cache/plan_county_year_2019.csv\n",
      "Wrote: data/processed/county_hhi_ma_share_2014_2019.csv rows: 19059\n"
     ]
    }
   ],
   "source": [
    "def build_county_hhi_ma_share_2014_2019(pen: pd.DataFrame) -> Path:\n",
    "    out_path = OUT_DIR / \"county_hhi_ma_share_2014_2019.csv\"\n",
    "\n",
    "    if FORCE_REBUILD_OUTPUTS and out_path.exists():\n",
    "        out_path.unlink()\n",
    "        print(\"Deleted output:\", out_path)\n",
    "\n",
    "    if out_path.exists():\n",
    "        return out_path\n",
    "\n",
    "    pen2 = pen.copy()\n",
    "    pen2[\"year\"] = pen2[\"year\"].astype(int)\n",
    "    pen2[\"fips\"] = pen2[\"fips\"].astype(str).str.zfill(5)\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for y in YEARS:\n",
    "        print(\"HHI + MA share. Year:\", y)\n",
    "        yp = build_plan_county_year(y)\n",
    "\n",
    "        usecols = [\"fips\", \"year\", \"contract_id\", \"dec_enrollment\", \"avg_enrollment\", \"drop_hw2\"]\n",
    "        contract_pieces = []\n",
    "\n",
    "        for chunk in pd.read_csv(yp, dtype=str, usecols=usecols, chunksize=CHUNKSIZE):\n",
    "            keep = chunk[\"drop_hw2\"].astype(str).str.strip().isin([\"0\", \"0.0\"])\n",
    "            chunk = chunk.loc[keep].copy()\n",
    "\n",
    "            chunk[\"fips\"] = chunk[\"fips\"].astype(str).str.zfill(5)\n",
    "            chunk[\"year\"] = pd.to_numeric(chunk[\"year\"], errors=\"coerce\").astype(int)\n",
    "            chunk[\"contract_id\"] = chunk[\"contract_id\"].astype(str).str.strip()\n",
    "\n",
    "            dec = pd.to_numeric(chunk[\"dec_enrollment\"], errors=\"coerce\")\n",
    "            avg = pd.to_numeric(chunk[\"avg_enrollment\"], errors=\"coerce\")\n",
    "            e = dec.fillna(avg).fillna(0.0)\n",
    "            chunk[\"e\"] = e\n",
    "\n",
    "            # IMPORTANT: HHI across firms => aggregate to contract within county-year first\n",
    "            tmp = (\n",
    "                chunk.groupby([\"fips\", \"year\", \"contract_id\"], as_index=False)\n",
    "                     .agg(contract_enroll=(\"e\", \"sum\"))\n",
    "            )\n",
    "            contract_pieces.append(tmp)\n",
    "\n",
    "            del chunk, tmp\n",
    "            gc.collect()\n",
    "\n",
    "        cdf = pd.concat(contract_pieces, ignore_index=True)\n",
    "        cdf = cdf.groupby([\"fips\", \"year\", \"contract_id\"], as_index=False)[\"contract_enroll\"].sum()\n",
    "\n",
    "        cdf[\"enroll_sq\"] = cdf[\"contract_enroll\"] * cdf[\"contract_enroll\"]\n",
    "\n",
    "        out = (\n",
    "            cdf.groupby([\"fips\", \"year\"], as_index=False)\n",
    "               .agg(\n",
    "                   ma_total_from_plans=(\"contract_enroll\", \"sum\"),\n",
    "                   sumsq_enroll=(\"enroll_sq\", \"sum\"),\n",
    "               )\n",
    "        )\n",
    "\n",
    "        out[\"hhi\"] = out[\"sumsq_enroll\"] / (out[\"ma_total_from_plans\"] * out[\"ma_total_from_plans\"]).replace({0: np.nan})\n",
    "\n",
    "        out = out.merge(pen2, on=[\"fips\", \"year\"], how=\"left\")\n",
    "        out[\"ma_share\"] = out[\"ma_enrollment\"] / out[\"eligibles\"]\n",
    "        out[\"ma_share_clipped\"] = out[\"ma_share\"].clip(lower=0, upper=1)\n",
    "\n",
    "        rows.append(out)\n",
    "\n",
    "        del contract_pieces, cdf, out\n",
    "        gc.collect()\n",
    "\n",
    "    final = pd.concat(rows, ignore_index=True).sort_values([\"year\",\"fips\"]).reset_index(drop=True)\n",
    "    final.to_csv(out_path, index=False)\n",
    "    print(\"Wrote:\", out_path, \"rows:\", len(final))\n",
    "    return out_path\n",
    "\n",
    "county_hhi_path = build_county_hhi_ma_share_2014_2019(pen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc39252a-12a4-4249-b94b-a5fdb76e16f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts rows: (19059, 3)\n",
      "HHI rows: (19059, 9)\n",
      "\n",
      "Plan count sanity:\n",
      "      count       mean  median  min  max\n",
      "year                                    \n",
      "2014   3162  23.867805    15.0    1  389\n",
      "2015   3169  24.770275    17.0    1  359\n",
      "2016   3175  26.181102    17.0    1  398\n",
      "2017   3172  26.918979    18.0    1  408\n",
      "2018   3185  32.416954    22.0    1  469\n",
      "2019   3196  36.135169    24.0    1  525\n",
      "\n",
      "HHI sanity:\n",
      "      count      mean    median       min  max\n",
      "year                                          \n",
      "2014   2980  0.452670  0.387899  0.101179  1.0\n",
      "2015   2977  0.450364  0.380093  0.089584  1.0\n",
      "2016   2988  0.453184  0.383756  0.103697  1.0\n",
      "2017   2987  0.459304  0.387529  0.091483  1.0\n",
      "2018   2994  0.440822  0.367966  0.089141  1.0\n",
      "2019   2987  0.398212  0.316754  0.086397  1.0\n",
      "\n",
      "MA share sanity:\n",
      "      count      mean    median       min       max\n",
      "year                                               \n",
      "2014   3108  0.224649  0.194643  0.004293  1.156381\n",
      "2015   3108  0.237273  0.210380  0.003261  1.152958\n",
      "2016   3114  0.243935  0.219293  0.003357  0.811015\n",
      "2017   3119  0.256713  0.234964  0.003108  0.819654\n",
      "2018   3124  0.262980  0.247459  0.004659  0.802589\n",
      "2019   3138  0.292419  0.288819  0.003145  1.091537\n",
      "\n",
      "Penetration merge missingness (ma_enrollment): 0.018259090193609318\n",
      "Penetration merge missingness (eligibles): 0.0015740594994490791\n",
      "Updated .gitignore with cache rules.\n"
     ]
    }
   ],
   "source": [
    "counts = pd.read_csv(OUT_DIR / \"county_plan_counts_2014_2019.csv\", dtype={\"fips\": str})\n",
    "hhi_df = pd.read_csv(OUT_DIR / \"county_hhi_ma_share_2014_2019.csv\", dtype={\"fips\": str})\n",
    "\n",
    "counts[\"fips\"] = counts[\"fips\"].str.zfill(5)\n",
    "counts[\"year\"] = counts[\"year\"].astype(int)\n",
    "\n",
    "hhi_df[\"fips\"] = hhi_df[\"fips\"].str.zfill(5)\n",
    "hhi_df[\"year\"] = hhi_df[\"year\"].astype(int)\n",
    "\n",
    "print(\"Counts rows:\", counts.shape)\n",
    "print(\"HHI rows:\", hhi_df.shape)\n",
    "\n",
    "print(\"\\nPlan count sanity:\")\n",
    "print(counts.groupby(\"year\")[\"plan_count\"].agg([\"count\",\"mean\",\"median\",\"min\",\"max\"]))\n",
    "\n",
    "hhi_df[\"hhi\"] = pd.to_numeric(hhi_df[\"hhi\"], errors=\"coerce\")\n",
    "hhi_df[\"ma_share\"] = pd.to_numeric(hhi_df[\"ma_share\"], errors=\"coerce\")\n",
    "\n",
    "print(\"\\nHHI sanity:\")\n",
    "print(hhi_df.groupby(\"year\")[\"hhi\"].agg([\"count\",\"mean\",\"median\",\"min\",\"max\"]))\n",
    "\n",
    "print(\"\\nMA share sanity:\")\n",
    "print(hhi_df.groupby(\"year\")[\"ma_share\"].agg([\"count\",\"mean\",\"median\",\"min\",\"max\"]))\n",
    "\n",
    "print(\"\\nPenetration merge missingness (ma_enrollment):\", hhi_df[\"ma_enrollment\"].isna().mean())\n",
    "print(\"Penetration merge missingness (eligibles):\", hhi_df[\"eligibles\"].isna().mean())\n",
    "\n",
    "# .gitignore: never push cache\n",
    "gitignore_path = Path(\".gitignore\")\n",
    "existing = gitignore_path.read_text().splitlines() if gitignore_path.exists() else []\n",
    "\n",
    "rules = [\n",
    "    \"data/cache/\",\n",
    "    \"data/cache/cms_payment_extracted/\",\n",
    "]\n",
    "\n",
    "new_lines = existing[:]\n",
    "for r in rules:\n",
    "    if r not in new_lines:\n",
    "        new_lines.append(r)\n",
    "\n",
    "gitignore_path.write_text(\"\\n\".join(new_lines) + \"\\n\")\n",
    "print(\"Updated .gitignore with cache rules.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
