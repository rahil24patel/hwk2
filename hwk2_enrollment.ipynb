{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "834c66d9-45f8-46df-a94f-208638830a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "YEARS = list(range(2014, 2020))\n",
    "\n",
    "CACHE_DIR = Path(\"data/cache\")\n",
    "OUT_DIR   = Path(\"data/processed\")\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHUNKSIZE = 300_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47b68fcf-6fc2-4e69-afd4-d70441d02bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MA_ROOT: /scion/5261/econ470001/ma-data/ma\n",
      "ENROLL_EXTRACTED exists: True\n",
      "SAREA_EXTRACTED exists: True\n",
      "PEN_EXTRACTED exists: True\n"
     ]
    }
   ],
   "source": [
    "CANDIDATE_MA_ROOTS = [\n",
    "    Path(\"/scion/5261/econ470001/ma-data/ma\"),\n",
    "    Path(\"/econ470/a0/work/ma-data/ma\"),\n",
    "    Path.cwd().parent / \"ma-data\" / \"ma\",\n",
    "]\n",
    "\n",
    "def pick_existing(paths: list[Path]) -> Path | None:\n",
    "    for p in paths:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "MA_ROOT = pick_existing(CANDIDATE_MA_ROOTS)\n",
    "if MA_ROOT is None:\n",
    "    raise FileNotFoundError(\"Could not find ma-data/ma. Update CANDIDATE_MA_ROOTS if needed.\")\n",
    "\n",
    "ENROLL_EXTRACTED = MA_ROOT / \"enrollment\" / \"Extracted Data\"\n",
    "SAREA_EXTRACTED  = MA_ROOT / \"service-area\" / \"Extracted Data\"\n",
    "PEN_EXTRACTED    = MA_ROOT / \"penetration\" / \"Extracted Data\"\n",
    "\n",
    "print(\"MA_ROOT:\", MA_ROOT)\n",
    "print(\"ENROLL_EXTRACTED exists:\", ENROLL_EXTRACTED.exists())\n",
    "print(\"SAREA_EXTRACTED exists:\", SAREA_EXTRACTED.exists())\n",
    "print(\"PEN_EXTRACTED exists:\", PEN_EXTRACTED.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a00fb9e-d310-4266-9463-3521f6dbd077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FORCE_REBUILD_CACHE: True\n",
      "FORCE_REBUILD_OUTPUTS: True\n"
     ]
    }
   ],
   "source": [
    "FORCE_REBUILD_CACHE   = True  \n",
    "FORCE_REBUILD_OUTPUTS = True   \n",
    "\n",
    "print(\"FORCE_REBUILD_CACHE:\", FORCE_REBUILD_CACHE)\n",
    "print(\"FORCE_REBUILD_OUTPUTS:\", FORCE_REBUILD_OUTPUTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2e9a4c2-12e9-43fc-8ba7-af1dd237b555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 6 cache files in data/cache/\n",
      "Deleted output: data/processed/county_plan_counts_2014_2019.csv\n",
      "Deleted output: data/processed/county_hhi_ma_share_2014_2019.csv\n"
     ]
    }
   ],
   "source": [
    "# One-time cleanup so you don't keep reading old cached/processed files\n",
    "if FORCE_REBUILD_CACHE:\n",
    "    n = 0\n",
    "    for p in CACHE_DIR.glob(\"plan_county_year_*.csv\"):\n",
    "        p.unlink()\n",
    "        n += 1\n",
    "    print(f\"Deleted {n} cache files in {CACHE_DIR}/\")\n",
    "\n",
    "if FORCE_REBUILD_OUTPUTS:\n",
    "    targets = [\n",
    "        OUT_DIR / \"county_plan_counts_2014_2019.csv\",\n",
    "        OUT_DIR / \"county_hhi_ma_share_2014_2019.csv\",\n",
    "    ]\n",
    "    for p in targets:\n",
    "        if p.exists():\n",
    "            p.unlink()\n",
    "            print(\"Deleted output:\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3ff8a2a-6ff5-48bd-a71b-d69f9a6f9d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRUE_SET  = {\"1\", \"1.0\", \"true\", \"t\", \"yes\", \"y\"}\n",
    "FALSE_SET = {\"0\", \"0.0\", \"false\", \"f\", \"no\", \"n\", \"nan\", \"none\", \"\"}\n",
    "\n",
    "def parse_boolish_series(x: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Robust bool parser: handles 0/1, True/False, \"1.0\", yes/no, etc.\n",
    "    Unknowns default to False.\n",
    "    \"\"\"\n",
    "    s = x.astype(str).str.strip().str.lower()\n",
    "    s = s.replace({\"<na>\": \"\", \"na\": \"\", \"n/a\": \"\"})\n",
    "\n",
    "    out = pd.Series(False, index=s.index)\n",
    "    out[s.isin(TRUE_SET)] = True\n",
    "    out[s.isin(FALSE_SET)] = False\n",
    "\n",
    "    # If numeric-ish, treat nonzero as True\n",
    "    num = pd.to_numeric(s, errors=\"coerce\")\n",
    "    out[num.notna()] = (num[num.notna()] != 0)\n",
    "    return out\n",
    "\n",
    "def norm_colname(s: str) -> str:\n",
    "    s = str(s).strip().lower()\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-z0-9_]\", \"\", s)\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s\n",
    "\n",
    "def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [norm_colname(c) for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def first_existing_col(cols: list[str], candidates: list[str]) -> str | None:\n",
    "    s = set(cols)\n",
    "    for c in candidates:\n",
    "        if c in s:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def coerce_id_series(x: pd.Series, width: int | None = None) -> pd.Series:\n",
    "    s = x.astype(str).str.strip()\n",
    "    s = s.replace({\"nan\": np.nan, \"None\": np.nan, \"\": np.nan})\n",
    "    s = s.str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "    if width is not None:\n",
    "        s = s.str.zfill(width)\n",
    "    return s\n",
    "\n",
    "def to_numeric_clean(x: pd.Series) -> pd.Series:\n",
    "    s = x.astype(str).str.replace(\",\", \"\", regex=False).str.strip()\n",
    "    s = s.replace({\"nan\": np.nan, \"None\": np.nan, \"\": np.nan, \"*\": np.nan, \"$-\": np.nan, \"-\": np.nan})\n",
    "    s = s.str.replace(\"$\", \"\", regex=False)\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "def _digits_zfill(x: pd.Series, width: int) -> pd.Series:\n",
    "    s = x.astype(str).str.strip()\n",
    "    s = s.replace({\"nan\": np.nan, \"None\": np.nan, \"\": np.nan})\n",
    "    s = s.str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "    s = s.str.replace(r\"[^0-9]\", \"\", regex=True)\n",
    "    s = s.where(s.str.len() > 0, np.nan)\n",
    "    return s.str.zfill(width)\n",
    "\n",
    "def coerce_fips(df: pd.DataFrame) -> pd.Series:\n",
    "    cols = list(df.columns)\n",
    "\n",
    "    fips_col = first_existing_col(\n",
    "        cols,\n",
    "        [\n",
    "            \"fips\",\n",
    "            \"fips_state_county_code\",\n",
    "            \"fipsstatecountycode\",\n",
    "            \"county_fips\",\n",
    "            \"countyfips\",\n",
    "            \"cnty_fips\",\n",
    "            \"fipscounty\",\n",
    "            \"fipscnty\",\n",
    "        ],\n",
    "    )\n",
    "    if fips_col is not None:\n",
    "        return _digits_zfill(df[fips_col], 5)\n",
    "\n",
    "    st_col = first_existing_col(cols, [\"state_fips\", \"statefips\", \"fipsst\", \"fips_state\"])\n",
    "    ct_col = first_existing_col(cols, [\"county_fips\", \"countyfips\", \"fipscnty\", \"fips_county\"])\n",
    "\n",
    "    if st_col is not None and ct_col is not None:\n",
    "        st = _digits_zfill(df[st_col], 2)\n",
    "        ct = _digits_zfill(df[ct_col], 3)\n",
    "        out = (st.fillna(\"\") + ct.fillna(\"\")).replace({\"\": np.nan})\n",
    "        return out\n",
    "\n",
    "    return pd.Series([np.nan] * len(df), index=df.index, dtype=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43d45d7f-af72-495c-b3ca-3a4f8ca9c95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_month_file(root: Path, year: int, month: int, kind: str) -> Path:\n",
    "    m2 = f\"{month:02d}\"\n",
    "    files = [p for p in root.rglob(\"*\") if p.is_file()]\n",
    "\n",
    "    if kind == \"enroll\":\n",
    "        pats = [rf\".*{year}.*{m2}.*enroll.*\\.(csv|txt)$\", rf\".*enroll.*{year}.*{m2}.*\\.(csv|txt)$\"]\n",
    "    elif kind == \"sarea\":\n",
    "        pats = [\n",
    "            rf\".*{year}.*{m2}.*(service|sa|sarea|cnty_sa|service_area).*?\\.(csv|txt)$\",\n",
    "            rf\".*(service|sa|sarea|cnty_sa|service_area).*{year}.*{m2}.*?\\.(csv|txt)$\",\n",
    "        ]\n",
    "    else:\n",
    "        raise ValueError(\"unknown kind\")\n",
    "\n",
    "    for pat in pats:\n",
    "        rx = re.compile(pat, flags=re.I)\n",
    "        hits = [p for p in files if rx.match(p.name)]\n",
    "        if hits:\n",
    "            hits.sort(key=lambda p: len(p.name))\n",
    "            return hits[0]\n",
    "\n",
    "    rx = re.compile(rf\".*{year}.*{m2}.*\\.(csv|txt)$\", flags=re.I)\n",
    "    hits = [p for p in files if rx.match(p.name)]\n",
    "    if hits:\n",
    "        hits.sort(key=lambda p: len(p.name))\n",
    "        return hits[0]\n",
    "\n",
    "    raise FileNotFoundError(f\"Could not find {kind} file for {year}-{m2} under {root}\")\n",
    "\n",
    "def find_year_file(root: Path, year: int, kind: str) -> Path:\n",
    "    files = [p for p in root.rglob(\"*\") if p.is_file()]\n",
    "    if kind != \"penetration\":\n",
    "        raise ValueError(\"unknown kind\")\n",
    "\n",
    "    pats = [rf\".*{year}.*penet.*\\.(csv|txt)$\", rf\".*penet.*{year}.*\\.(csv|txt)$\"]\n",
    "    for pat in pats:\n",
    "        rx = re.compile(pat, flags=re.I)\n",
    "        hits = [p for p in files if rx.match(p.name)]\n",
    "        if hits:\n",
    "            hits.sort(key=lambda p: len(p.name))\n",
    "            return hits[0]\n",
    "\n",
    "    rx = re.compile(rf\".*{year}.*\\.(csv|txt)$\", flags=re.I)\n",
    "    hits = [p for p in files if rx.match(p.name)]\n",
    "    if hits:\n",
    "        hits.sort(key=lambda p: len(p.name))\n",
    "        return hits[0]\n",
    "\n",
    "    raise FileNotFoundError(f\"Could not find {kind} file for {year} under {root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba69d580-3cd5-4903-a50b-e1cf0a156cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_any(path: Path, usecols=None) -> pd.DataFrame:\n",
    "    return pd.read_csv(\n",
    "        path,\n",
    "        dtype=str,\n",
    "        na_values=[\"*\", \"NA\", \"N/A\", \"\"],\n",
    "        keep_default_na=True,\n",
    "        encoding_errors=\"replace\",\n",
    "        low_memory=False,\n",
    "        usecols=usecols,\n",
    "    )\n",
    "\n",
    "def read_enroll_min(path: Path) -> pd.DataFrame:\n",
    "    want = {\n",
    "        \"contract_number\",\n",
    "        \"contract_id\",\n",
    "        \"plan_id\",\n",
    "        \"fips_state_county_code\",\n",
    "        \"enrollment\",\n",
    "        \"snp\",\n",
    "        \"snp_type\",\n",
    "        \"partc\",\n",
    "        \"part_c\",\n",
    "        \"plan_type\",\n",
    "        \"contract_type\",\n",
    "    }\n",
    "\n",
    "    df = read_csv_any(path, usecols=lambda c: norm_colname(c) in want)\n",
    "    df = normalize_columns(df)\n",
    "\n",
    "    contract_col = first_existing_col(list(df.columns), [\"contract_id\", \"contract_number\"])\n",
    "    if contract_col is None:\n",
    "        raise KeyError(\"Enrollment file missing contract column\")\n",
    "    if \"plan_id\" not in df.columns:\n",
    "        raise KeyError(\"Enrollment file missing plan_id column\")\n",
    "    if \"enrollment\" not in df.columns:\n",
    "        raise KeyError(\"Enrollment file missing enrollment column\")\n",
    "\n",
    "    df[\"contract_id\"] = coerce_id_series(df[contract_col], None)\n",
    "    df[\"plan_id\"] = coerce_id_series(df[\"plan_id\"], 3)\n",
    "\n",
    "    fips_src = df.get(\"fips_state_county_code\", pd.Series([np.nan] * len(df), index=df.index))\n",
    "    df[\"fips\"] = _digits_zfill(fips_src, 5)\n",
    "\n",
    "    df[\"enrollment\"] = to_numeric_clean(df[\"enrollment\"])\n",
    "\n",
    "    keep = [\"contract_id\", \"plan_id\", \"fips\", \"enrollment\"]\n",
    "    for c in [\"snp\", \"snp_type\", \"partc\", \"part_c\", \"plan_type\", \"contract_type\"]:\n",
    "        if c in df.columns:\n",
    "            keep.append(c)\n",
    "\n",
    "    return df[keep]\n",
    "\n",
    "def read_sarea_min(path: Path) -> pd.DataFrame:\n",
    "    keep = {\n",
    "        \"contract_id\",\n",
    "        \"contract_number\",\n",
    "        \"plan_id\",\n",
    "        \"plan\",\n",
    "        \"planid\",\n",
    "        \"plan_number\",\n",
    "        \"fips\",\n",
    "        \"fips_state_county_code\",\n",
    "        \"fipscounty\",\n",
    "        \"fipscnty\",\n",
    "        \"county_fips\",\n",
    "        \"cnty_fips\",\n",
    "    }\n",
    "\n",
    "    df = read_csv_any(path, usecols=lambda c: norm_colname(c) in keep)\n",
    "    df = normalize_columns(df)\n",
    "\n",
    "    cols = list(df.columns)\n",
    "    contract_col = first_existing_col(cols, [\"contract_id\", \"contract_number\", \"contract\"])\n",
    "    if contract_col is None:\n",
    "        raise KeyError(f\"Service area missing contract column. Sample cols: {sorted(cols)[:60]}\")\n",
    "\n",
    "    df[\"contract_id\"] = coerce_id_series(df[contract_col], None)\n",
    "\n",
    "    plan_col = first_existing_col(cols, [\"plan_id\", \"planid\", \"plan\", \"plan_number\"])\n",
    "    if plan_col is None:\n",
    "        df[\"plan_id\"] = np.nan\n",
    "    else:\n",
    "        df[\"plan_id\"] = coerce_id_series(df[plan_col], 3)\n",
    "\n",
    "    df[\"fips\"] = coerce_fips(df)\n",
    "\n",
    "    return df[[\"contract_id\", \"plan_id\", \"fips\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c0cea0c-94fe-442b-b091-df40972c3ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_exclusions(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Produces:\n",
    "      is_snp, is_800_series, is_pdp_only, drop_hw2  (all 0/1 ints)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # plan id + 800-series\n",
    "    df[\"plan_id\"] = df[\"plan_id\"].astype(str).str.strip().str.zfill(3)\n",
    "    df[\"is_800_series\"] = df[\"plan_id\"].str.startswith(\"8\").astype(int)\n",
    "\n",
    "    cols = list(df.columns)\n",
    "\n",
    "    # SNP\n",
    "    snp_col = first_existing_col(cols, [\"snp\"])\n",
    "    snp_type_col = first_existing_col(cols, [\"snp_type\"])\n",
    "\n",
    "    if snp_col is not None:\n",
    "        df[\"is_snp\"] = parse_boolish_series(df[snp_col]).astype(int)\n",
    "    elif snp_type_col is not None:\n",
    "        st = df[snp_type_col].astype(str).str.strip().str.lower()\n",
    "        df[\"is_snp\"] = st.str.contains(\"snp\", na=False).astype(int)\n",
    "    else:\n",
    "        df[\"is_snp\"] = 0\n",
    "\n",
    "    # PDP-only\n",
    "    partc_col = first_existing_col(cols, [\"partc\", \"part_c\"])\n",
    "    if partc_col is not None:\n",
    "        is_partc = parse_boolish_series(df[partc_col])\n",
    "        df[\"is_pdp_only\"] = (~is_partc).astype(int)\n",
    "    else:\n",
    "        plan_type_col = first_existing_col(cols, [\"plan_type\", \"contract_type\"])\n",
    "        if plan_type_col is None:\n",
    "            df[\"is_pdp_only\"] = 0\n",
    "        else:\n",
    "            s = df[plan_type_col].astype(str).str.lower().str.strip()\n",
    "            df[\"is_pdp_only\"] = (\n",
    "                s.str.contains(r\"\\bpdp\\b\", regex=True, na=False)\n",
    "                | s.str.contains(\"prescription drug\", na=False)\n",
    "                | s.str.contains(\"part d\", na=False)\n",
    "            ).astype(int)\n",
    "\n",
    "    df[\"drop_hw2\"] = ((df[\"is_snp\"] == 1) | (df[\"is_800_series\"] == 1) | (df[\"is_pdp_only\"] == 1)).astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb1ddb98-0ccc-40f9-b61b-1c75596f7f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_plan_county_year(year: int) -> Path:\n",
    "    out_path = CACHE_DIR / f\"plan_county_year_{year}.csv\"\n",
    "\n",
    "    if FORCE_REBUILD_CACHE and out_path.exists():\n",
    "        out_path.unlink()\n",
    "        print(\"Deleted cache:\", out_path)\n",
    "\n",
    "    if out_path.exists():\n",
    "        return out_path\n",
    "\n",
    "    accum = None\n",
    "\n",
    "    for m in range(1, 13):\n",
    "        enroll_path = find_month_file(ENROLL_EXTRACTED, year, m, \"enroll\")\n",
    "        sarea_path  = find_month_file(SAREA_EXTRACTED,  year, m, \"sarea\")\n",
    "\n",
    "        enroll = read_enroll_min(enroll_path)\n",
    "        sarea  = read_sarea_min(sarea_path)\n",
    "\n",
    "        enroll = enroll.dropna(subset=[\"contract_id\", \"plan_id\", \"fips\"])\n",
    "        sarea  = sarea.dropna(subset=[\"contract_id\", \"fips\"])\n",
    "\n",
    "        # If service-area includes plan_id, merge on contract-plan-fips; else merge contract-fips only\n",
    "        has_plan = sarea[\"plan_id\"].notna().any()\n",
    "        if has_plan:\n",
    "            sarea = sarea.dropna(subset=[\"plan_id\"])\n",
    "            merged = enroll.merge(sarea, on=[\"contract_id\", \"plan_id\", \"fips\"], how=\"inner\")\n",
    "        else:\n",
    "            merged = enroll.merge(sarea[[\"contract_id\", \"fips\"]], on=[\"contract_id\", \"fips\"], how=\"inner\")\n",
    "\n",
    "        merged[\"year\"] = int(year)\n",
    "        merged = flag_exclusions(merged)\n",
    "\n",
    "        # collapse within month\n",
    "        g = merged.groupby([\"contract_id\", \"plan_id\", \"fips\", \"year\"], as_index=False).agg(\n",
    "            enroll_sum=(\"enrollment\", \"sum\"),\n",
    "            is_snp=(\"is_snp\", \"max\"),\n",
    "            is_800_series=(\"is_800_series\", \"max\"),\n",
    "            is_pdp_only=(\"is_pdp_only\", \"max\"),\n",
    "            drop_hw2=(\"drop_hw2\", \"max\"),\n",
    "        )\n",
    "\n",
    "        g[\"months_observed\"] = 1\n",
    "        g[\"dec_enrollment\"] = g[\"enroll_sum\"] if m == 12 else np.nan\n",
    "\n",
    "        if accum is None:\n",
    "            accum = g\n",
    "        else:\n",
    "            keys = [\"contract_id\", \"plan_id\", \"fips\", \"year\"]\n",
    "            accum = accum.merge(g, on=keys, how=\"outer\", suffixes=(\"\", \"_new\"))\n",
    "\n",
    "            accum[\"enroll_sum\"] = accum[\"enroll_sum\"].fillna(0) + accum[\"enroll_sum_new\"].fillna(0)\n",
    "            accum[\"months_observed\"] = accum[\"months_observed\"].fillna(0) + accum[\"months_observed_new\"].fillna(0)\n",
    "\n",
    "            for c in [\"is_snp\", \"is_800_series\", \"is_pdp_only\", \"drop_hw2\"]:\n",
    "                accum[c] = np.maximum(\n",
    "                    pd.to_numeric(accum[c], errors=\"coerce\").fillna(0),\n",
    "                    pd.to_numeric(accum[f\"{c}_new\"], errors=\"coerce\").fillna(0),\n",
    "                ).astype(int)\n",
    "\n",
    "            accum[\"dec_enrollment\"] = accum[\"dec_enrollment\"].combine_first(accum[\"dec_enrollment_new\"])\n",
    "\n",
    "            dropcols = [c for c in accum.columns if c.endswith(\"_new\")]\n",
    "            accum = accum.drop(columns=dropcols)\n",
    "\n",
    "        del enroll, sarea, merged, g\n",
    "        gc.collect()\n",
    "\n",
    "    accum[\"avg_enrollment\"] = accum[\"enroll_sum\"] / accum[\"months_observed\"].replace({0: np.nan})\n",
    "\n",
    "    out = accum[\n",
    "        [\n",
    "            \"contract_id\",\n",
    "            \"plan_id\",\n",
    "            \"fips\",\n",
    "            \"year\",\n",
    "            \"avg_enrollment\",\n",
    "            \"dec_enrollment\",\n",
    "            \"months_observed\",\n",
    "            \"is_snp\",\n",
    "            \"is_800_series\",\n",
    "            \"is_pdp_only\",\n",
    "            \"drop_hw2\",\n",
    "        ]\n",
    "    ].copy()\n",
    "\n",
    "    # enforce clean types for downstream notebooks\n",
    "    out[\"contract_id\"] = out[\"contract_id\"].astype(str).str.strip()\n",
    "    out[\"plan_id\"] = out[\"plan_id\"].astype(str).str.strip().str.zfill(3)\n",
    "    out[\"fips\"] = out[\"fips\"].astype(str).str.strip().str.zfill(5)\n",
    "    out[\"year\"] = pd.to_numeric(out[\"year\"], errors=\"coerce\").astype(int)\n",
    "\n",
    "    for c in [\"is_snp\", \"is_800_series\", \"is_pdp_only\", \"drop_hw2\"]:\n",
    "        out[c] = pd.to_numeric(out[c], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "    for c in [\"avg_enrollment\", \"dec_enrollment\", \"months_observed\"]:\n",
    "        out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "\n",
    "    out.to_csv(out_path, index=False)\n",
    "\n",
    "    del accum, out\n",
    "    gc.collect()\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09b4ffab-f0ec-4382-bc70-26f38869ed93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built / found: data/cache/plan_county_year_2018.csv\n",
      "Rows: 1366487\n",
      "Unique counties: 3225\n",
      "Drop rate (drop_hw2): 0.9244427499127325\n",
      "\n",
      "2018 plan counts (contract-plan) summary:\n",
      "count    3185.000000\n",
      "mean       32.416954\n",
      "std        39.516647\n",
      "min         1.000000\n",
      "5%          4.000000\n",
      "25%        12.000000\n",
      "50%        22.000000\n",
      "75%        39.000000\n",
      "95%        96.000000\n",
      "max       469.000000\n",
      "Name: plan_key, dtype: float64\n",
      "\n",
      "2018 contract counts (firms) summary:\n",
      "count    3185.000000\n",
      "mean       11.720879\n",
      "std        10.009149\n",
      "min         1.000000\n",
      "5%          2.000000\n",
      "25%         5.000000\n",
      "50%         9.000000\n",
      "75%        14.000000\n",
      "95%        31.000000\n",
      "max        82.000000\n",
      "Name: contract_id, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "yp = build_plan_county_year(2018)\n",
    "print(\"Built / found:\", yp)\n",
    "\n",
    "df = pd.read_csv(yp, dtype=str)\n",
    "df[\"plan_id\"] = df[\"plan_id\"].str.zfill(3)\n",
    "df[\"fips\"] = df[\"fips\"].str.zfill(5)\n",
    "\n",
    "print(\"Rows:\", df.shape[0])\n",
    "print(\"Unique counties:\", df[\"fips\"].nunique())\n",
    "\n",
    "keep = df[\"drop_hw2\"].astype(str).str.strip().isin([\"0\", \"0.0\"])\n",
    "print(\"Drop rate (drop_hw2):\", (~keep).mean())\n",
    "\n",
    "df_nodrop = df.loc[keep].copy()\n",
    "df_nodrop[\"plan_key\"] = df_nodrop[\"contract_id\"].str.strip() + \"-\" + df_nodrop[\"plan_id\"].str.strip()\n",
    "\n",
    "plan_ct = df_nodrop.groupby([\"fips\", \"year\"])[\"plan_key\"].nunique()\n",
    "contract_ct = df_nodrop.groupby([\"fips\", \"year\"])[\"contract_id\"].nunique()\n",
    "\n",
    "print(\"\\n2018 plan counts (contract-plan) summary:\")\n",
    "print(plan_ct.describe(percentiles=[0.05,0.25,0.5,0.75,0.95]))\n",
    "\n",
    "print(\"\\n2018 contract counts (firms) summary:\")\n",
    "print(contract_ct.describe(percentiles=[0.05,0.25,0.5,0.75,0.95]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf61494b-b794-4fd8-928a-f8a2edfdbc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting unique plans. Year: 2014\n",
      "Counting unique plans. Year: 2015\n",
      "Counting unique plans. Year: 2016\n",
      "Counting unique plans. Year: 2017\n",
      "Counting unique plans. Year: 2018\n",
      "Deleted cache: data/cache/plan_county_year_2018.csv\n",
      "Counting unique plans. Year: 2019\n",
      "Wrote: data/processed/county_plan_counts_2014_2019.csv\n",
      "\n",
      "Plan count summary by year:\n",
      "       count       mean        std  min   5%   25%   50%   75%    95%    max\n",
      "year                                                                        \n",
      "2014  3162.0  23.867805  29.273318  1.0  3.0   9.0  15.0  29.0   65.0  389.0\n",
      "2015  3169.0  24.770275  29.153822  1.0  4.0  10.0  17.0  30.0   71.0  359.0\n",
      "2016  3175.0  26.181102  31.409309  1.0  4.0  10.0  17.0  31.5   78.0  398.0\n",
      "2017  3172.0  26.918979  32.100049  1.0  4.0  10.0  18.0  32.0   78.0  408.0\n",
      "2018  3185.0  32.416954  39.516647  1.0  4.0  12.0  22.0  39.0   96.0  469.0\n",
      "2019  3196.0  36.135169  43.513600  1.0  4.0  14.0  24.0  43.0  105.0  525.0\n",
      "\n",
      "Medians by year:\n",
      "year\n",
      "2014    15.0\n",
      "2015    17.0\n",
      "2016    17.0\n",
      "2017    18.0\n",
      "2018    22.0\n",
      "2019    24.0\n",
      "Name: plan_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def build_county_plan_counts_2014_2019() -> Path:\n",
    "    out_path = OUT_DIR / \"county_plan_counts_2014_2019.csv\"\n",
    "\n",
    "    if FORCE_REBUILD_OUTPUTS and out_path.exists():\n",
    "        out_path.unlink()\n",
    "        print(\"Deleted output:\", out_path)\n",
    "\n",
    "    # IMPORTANT: after cleanup, this should NOT exist\n",
    "    if out_path.exists():\n",
    "        print(\"Existing file:\", out_path)\n",
    "        return out_path\n",
    "\n",
    "    counts_sets: dict[tuple[str, int], set[str]] = {}\n",
    "\n",
    "    for y in YEARS:\n",
    "        print(\"Counting unique plans. Year:\", y)\n",
    "        yp = build_plan_county_year(y)\n",
    "\n",
    "        usecols = [\"fips\", \"year\", \"contract_id\", \"plan_id\", \"drop_hw2\"]\n",
    "        for chunk in pd.read_csv(yp, dtype=str, usecols=usecols, chunksize=CHUNKSIZE):\n",
    "            keep = chunk[\"drop_hw2\"].astype(str).str.strip().isin([\"0\", \"0.0\"])\n",
    "            chunk = chunk.loc[keep, [\"fips\", \"year\", \"contract_id\", \"plan_id\"]].dropna()\n",
    "\n",
    "            chunk[\"fips\"] = chunk[\"fips\"].astype(str).str.zfill(5)\n",
    "            chunk[\"year\"] = pd.to_numeric(chunk[\"year\"], errors=\"coerce\").astype(int)\n",
    "            chunk[\"plan_id\"] = chunk[\"plan_id\"].astype(str).str.zfill(3)\n",
    "\n",
    "            chunk[\"plan_key\"] = chunk[\"contract_id\"].astype(str).str.strip() + \"-\" + chunk[\"plan_id\"].astype(str).str.strip()\n",
    "\n",
    "            for (fips, year), g in chunk.groupby([\"fips\", \"year\"]):\n",
    "                key = (str(fips), int(year))\n",
    "                counts_sets.setdefault(key, set()).update(g[\"plan_key\"].unique().tolist())\n",
    "\n",
    "            del chunk\n",
    "            gc.collect()\n",
    "\n",
    "    out = pd.DataFrame(\n",
    "        [(fips, year, len(s)) for (fips, year), s in counts_sets.items()],\n",
    "        columns=[\"fips\", \"year\", \"plan_count\"]\n",
    "    ).sort_values([\"year\",\"fips\"]).reset_index(drop=True)\n",
    "\n",
    "    out.to_csv(out_path, index=False)\n",
    "    return out_path\n",
    "\n",
    "county_counts_path = build_county_plan_counts_2014_2019()\n",
    "print(\"Wrote:\", county_counts_path)\n",
    "\n",
    "counts = pd.read_csv(county_counts_path, dtype={\"fips\": str})\n",
    "counts[\"fips\"] = counts[\"fips\"].str.zfill(5)\n",
    "counts[\"year\"] = counts[\"year\"].astype(int)\n",
    "\n",
    "print(\"\\nPlan count summary by year:\")\n",
    "print(counts.groupby(\"year\")[\"plan_count\"].describe(percentiles=[0.05,0.25,0.5,0.75,0.95]))\n",
    "\n",
    "print(\"\\nMedians by year:\")\n",
    "print(counts.groupby(\"year\")[\"plan_count\"].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8acaf44c-e39e-4bb7-b896-dc254f358cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penetration shape: (19494, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fips</th>\n",
       "      <th>ma_enrollment</th>\n",
       "      <th>eligibles</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01001</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>9438</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01003</td>\n",
       "      <td>13183.0</td>\n",
       "      <td>41640</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01005</td>\n",
       "      <td>859.0</td>\n",
       "      <td>6004</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01007</td>\n",
       "      <td>1665.0</td>\n",
       "      <td>4599</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01009</td>\n",
       "      <td>4680.0</td>\n",
       "      <td>11193</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fips  ma_enrollment  eligibles  year\n",
       "0  01001         3000.0       9438  2014\n",
       "1  01003        13183.0      41640  2014\n",
       "2  01005          859.0       6004  2014\n",
       "3  01007         1665.0       4599  2014\n",
       "4  01009         4680.0      11193  2014"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_penetration_year(year: int) -> pd.DataFrame:\n",
    "    path = find_year_file(PEN_EXTRACTED, year, \"penetration\")\n",
    "    df = read_csv_any(path)\n",
    "    df = normalize_columns(df)\n",
    "\n",
    "    df[\"fips\"] = coerce_fips(df)\n",
    "\n",
    "    cols = list(df.columns)\n",
    "    ma_col = first_existing_col(cols, [\"ma_enrollment\", \"ma_enroll\", \"enrolled\", \"enroll\"])\n",
    "    elig_col = first_existing_col(cols, [\"eligibles\", \"total_eligibles\", \"medicare_eligibles\", \"tot_eligibles\"])\n",
    "\n",
    "    if ma_col is None or elig_col is None:\n",
    "        raise KeyError(f\"Penetration file missing enrolled or eligibles. Col sample: {sorted(cols)[:100]}\")\n",
    "\n",
    "    out = df[[\"fips\", ma_col, elig_col]].copy()\n",
    "    out = out.rename(columns={ma_col: \"ma_enrollment\", elig_col: \"eligibles\"})\n",
    "    out[\"ma_enrollment\"] = to_numeric_clean(out[\"ma_enrollment\"])\n",
    "    out[\"eligibles\"] = to_numeric_clean(out[\"eligibles\"])\n",
    "    out[\"year\"] = int(year)\n",
    "\n",
    "    out = out.dropna(subset=[\"fips\"])\n",
    "    out[\"fips\"] = out[\"fips\"].astype(str).str.zfill(5)\n",
    "    return out\n",
    "\n",
    "pen = pd.concat([read_penetration_year(y) for y in YEARS], ignore_index=True)\n",
    "print(\"Penetration shape:\", pen.shape)\n",
    "pen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d089e24-14b4-44a7-9048-53feca85e26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HHI and MA share. Year: 2014\n",
      "Deleted cache: data/cache/plan_county_year_2014.csv\n",
      "HHI and MA share. Year: 2015\n",
      "Deleted cache: data/cache/plan_county_year_2015.csv\n",
      "HHI and MA share. Year: 2016\n",
      "Deleted cache: data/cache/plan_county_year_2016.csv\n",
      "HHI and MA share. Year: 2017\n",
      "Deleted cache: data/cache/plan_county_year_2017.csv\n",
      "HHI and MA share. Year: 2018\n",
      "Deleted cache: data/cache/plan_county_year_2018.csv\n",
      "HHI and MA share. Year: 2019\n",
      "Deleted cache: data/cache/plan_county_year_2019.csv\n",
      "Wrote: data/processed/county_hhi_ma_share_2014_2019.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fips</th>\n",
       "      <th>year</th>\n",
       "      <th>ma_total_from_plans</th>\n",
       "      <th>sumsq_enroll</th>\n",
       "      <th>hhi</th>\n",
       "      <th>ma_enrollment</th>\n",
       "      <th>eligibles</th>\n",
       "      <th>ma_share</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01001</td>\n",
       "      <td>2014</td>\n",
       "      <td>2996.0</td>\n",
       "      <td>1222586.0</td>\n",
       "      <td>0.136206</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>9438.0</td>\n",
       "      <td>0.317864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01003</td>\n",
       "      <td>2014</td>\n",
       "      <td>12948.0</td>\n",
       "      <td>27882662.0</td>\n",
       "      <td>0.166314</td>\n",
       "      <td>13183.0</td>\n",
       "      <td>41640.0</td>\n",
       "      <td>0.316595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01005</td>\n",
       "      <td>2014</td>\n",
       "      <td>844.0</td>\n",
       "      <td>251720.0</td>\n",
       "      <td>0.353373</td>\n",
       "      <td>859.0</td>\n",
       "      <td>6004.0</td>\n",
       "      <td>0.143071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01007</td>\n",
       "      <td>2014</td>\n",
       "      <td>1596.0</td>\n",
       "      <td>481766.0</td>\n",
       "      <td>0.189134</td>\n",
       "      <td>1665.0</td>\n",
       "      <td>4599.0</td>\n",
       "      <td>0.362035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01009</td>\n",
       "      <td>2014</td>\n",
       "      <td>4511.0</td>\n",
       "      <td>3401253.0</td>\n",
       "      <td>0.167145</td>\n",
       "      <td>4680.0</td>\n",
       "      <td>11193.0</td>\n",
       "      <td>0.418118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fips  year  ma_total_from_plans  sumsq_enroll       hhi  ma_enrollment  eligibles  ma_share\n",
       "0  01001  2014               2996.0     1222586.0  0.136206         3000.0     9438.0  0.317864\n",
       "1  01003  2014              12948.0    27882662.0  0.166314        13183.0    41640.0  0.316595\n",
       "2  01005  2014                844.0      251720.0  0.353373          859.0     6004.0  0.143071\n",
       "3  01007  2014               1596.0      481766.0  0.189134         1665.0     4599.0  0.362035\n",
       "4  01009  2014               4511.0     3401253.0  0.167145         4680.0    11193.0  0.418118"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_county_hhi_ma_share_2014_2019(pen: pd.DataFrame) -> Path:\n",
    "    out_path = OUT_DIR / \"county_hhi_ma_share_2014_2019.csv\"\n",
    "\n",
    "    if FORCE_REBUILD_OUTPUTS and out_path.exists():\n",
    "        out_path.unlink()\n",
    "        print(\"Deleted output:\", out_path)\n",
    "\n",
    "    if out_path.exists():\n",
    "        print(\"Existing file:\", out_path)\n",
    "        return out_path\n",
    "\n",
    "    pen2 = pen.copy()\n",
    "    pen2[\"year\"] = pen2[\"year\"].astype(int)\n",
    "    pen2[\"fips\"] = pen2[\"fips\"].astype(str).str.zfill(5)\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for y in YEARS:\n",
    "        print(\"HHI and MA share. Year:\", y)\n",
    "        yp = build_plan_county_year(y)\n",
    "\n",
    "        usecols = [\"fips\", \"year\", \"dec_enrollment\", \"avg_enrollment\", \"drop_hw2\"]\n",
    "        pieces = []\n",
    "\n",
    "        for chunk in pd.read_csv(yp, dtype=str, usecols=usecols, chunksize=CHUNKSIZE):\n",
    "            keep = chunk[\"drop_hw2\"].astype(str).str.strip().isin([\"0\", \"0.0\"])\n",
    "            chunk = chunk.loc[keep].copy()\n",
    "\n",
    "            dec = pd.to_numeric(chunk[\"dec_enrollment\"], errors=\"coerce\")\n",
    "            avg = pd.to_numeric(chunk[\"avg_enrollment\"], errors=\"coerce\")\n",
    "            e = dec.fillna(avg).fillna(0.0)\n",
    "\n",
    "            chunk[\"e\"] = e\n",
    "            chunk[\"e2\"] = e * e\n",
    "\n",
    "            tmp = (\n",
    "                chunk.groupby([\"fips\", \"year\"], as_index=False)\n",
    "                     .agg(total_enroll=(\"e\", \"sum\"), sumsq_enroll=(\"e2\", \"sum\"))\n",
    "            )\n",
    "            pieces.append(tmp)\n",
    "\n",
    "            del chunk, tmp\n",
    "            gc.collect()\n",
    "\n",
    "        out = pd.concat(pieces, ignore_index=True)\n",
    "        out = out.groupby([\"fips\", \"year\"], as_index=False).agg(\n",
    "            total_enroll=(\"total_enroll\", \"sum\"),\n",
    "            sumsq_enroll=(\"sumsq_enroll\", \"sum\"),\n",
    "        )\n",
    "\n",
    "        out[\"hhi\"] = out[\"sumsq_enroll\"] / (out[\"total_enroll\"] * out[\"total_enroll\"])\n",
    "        out = out.rename(columns={\"total_enroll\": \"ma_total_from_plans\"})\n",
    "        out[\"fips\"] = out[\"fips\"].astype(str).str.zfill(5)\n",
    "        out[\"year\"] = out[\"year\"].astype(int)\n",
    "\n",
    "        out = out.merge(pen2, on=[\"fips\", \"year\"], how=\"left\")\n",
    "        out[\"ma_share\"] = out[\"ma_enrollment\"] / out[\"eligibles\"]\n",
    "\n",
    "        rows.append(out)\n",
    "\n",
    "        del out, pieces\n",
    "        gc.collect()\n",
    "\n",
    "    final = pd.concat(rows, ignore_index=True).sort_values([\"year\", \"fips\"]).reset_index(drop=True)\n",
    "    final.to_csv(out_path, index=False)\n",
    "    return out_path\n",
    "\n",
    "county_hhi_path = build_county_hhi_ma_share_2014_2019(pen)\n",
    "print(\"Wrote:\", county_hhi_path)\n",
    "\n",
    "pd.read_csv(county_hhi_path, dtype={\"fips\": str}, nrows=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef5d8924-81e0-4a30-a85c-41a39e16c497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts rows: (19059, 3)\n",
      "HHI rows: (19059, 8)\n",
      "\n",
      "Plan count sanity:\n",
      "      count       mean  median  min  max\n",
      "year                                    \n",
      "2014   3162  23.867805    15.0    1  389\n",
      "2015   3169  24.770275    17.0    1  359\n",
      "2016   3175  26.181102    17.0    1  398\n",
      "2017   3172  26.918979    18.0    1  408\n",
      "2018   3185  32.416954    22.0    1  469\n",
      "2019   3196  36.135169    24.0    1  525\n",
      "\n",
      "HHI sanity:\n",
      "      count      mean    median       min  max\n",
      "year                                          \n",
      "2014   2980  0.323920  0.244332  0.051506  1.0\n",
      "2015   2977  0.318471  0.238641  0.047353  1.0\n",
      "2016   2988  0.315634  0.233470  0.051027  1.0\n",
      "2017   2987  0.312165  0.228132  0.055574  1.0\n",
      "2018   2994  0.286846  0.212237  0.046200  1.0\n",
      "2019   2987  0.251319  0.182044  0.039916  1.0\n",
      "\n",
      "MA share sanity:\n",
      "      count      mean    median       min       max\n",
      "year                                               \n",
      "2014   3108  0.224649  0.194643  0.004293  1.156381\n",
      "2015   3108  0.237273  0.210380  0.003261  1.152958\n",
      "2016   3114  0.243935  0.219293  0.003357  0.811015\n",
      "2017   3119  0.256713  0.234964  0.003108  0.819654\n",
      "2018   3124  0.262980  0.247459  0.004659  0.802589\n",
      "2019   3138  0.292419  0.288819  0.003145  1.091537\n",
      "\n",
      "Penetration merge missingness (ma_enrollment): 0.018259090193609318\n",
      "Penetration merge missingness (eligibles): 0.0015740594994490791\n"
     ]
    }
   ],
   "source": [
    "counts = pd.read_csv(OUT_DIR / \"county_plan_counts_2014_2019.csv\")\n",
    "hhi_df = pd.read_csv(OUT_DIR / \"county_hhi_ma_share_2014_2019.csv\")\n",
    "\n",
    "print(\"Counts rows:\", counts.shape)\n",
    "print(\"HHI rows:\", hhi_df.shape)\n",
    "\n",
    "print(\"\\nPlan count sanity:\")\n",
    "print(counts.groupby(\"year\")[\"plan_count\"].agg([\"count\",\"mean\",\"median\",\"min\",\"max\"]))\n",
    "\n",
    "print(\"\\nHHI sanity:\")\n",
    "hhi_df[\"hhi\"] = pd.to_numeric(hhi_df[\"hhi\"], errors=\"coerce\")\n",
    "print(hhi_df.groupby(\"year\")[\"hhi\"].agg([\"count\",\"mean\",\"median\",\"min\",\"max\"]))\n",
    "\n",
    "print(\"\\nMA share sanity:\")\n",
    "hhi_df[\"ma_share\"] = pd.to_numeric(hhi_df[\"ma_share\"], errors=\"coerce\")\n",
    "print(hhi_df.groupby(\"year\")[\"ma_share\"].agg([\"count\",\"mean\",\"median\",\"min\",\"max\"]))\n",
    "\n",
    "print(\"\\nPenetration merge missingness (ma_enrollment):\", hhi_df[\"ma_enrollment\"].isna().mean())\n",
    "print(\"Penetration merge missingness (eligibles):\", hhi_df[\"eligibles\"].isna().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa8a9165-b70b-4881-aff7-714e0e8f7548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated .gitignore\n",
      "Keep these for HW2 analysis:\n",
      "  data/processed/county_plan_counts_2014_2019.csv\n",
      "  data/processed/county_hhi_ma_share_2014_2019.csv\n",
      "Do not push data/cache/\n"
     ]
    }
   ],
   "source": [
    "gitignore_path = Path(\".gitignore\")\n",
    "existing = gitignore_path.read_text().splitlines() if gitignore_path.exists() else []\n",
    "\n",
    "rules = [\n",
    "    \"data/cache/\",\n",
    "]\n",
    "\n",
    "new_lines = existing[:]\n",
    "for r in rules:\n",
    "    if r not in new_lines:\n",
    "        new_lines.append(r)\n",
    "\n",
    "gitignore_path.write_text(\"\\n\".join(new_lines) + \"\\n\")\n",
    "\n",
    "print(\"Updated .gitignore\")\n",
    "print(\"Keep these for HW2 analysis:\")\n",
    "print(\" \", OUT_DIR / \"county_plan_counts_2014_2019.csv\")\n",
    "print(\" \", OUT_DIR / \"county_hhi_ma_share_2014_2019.csv\")\n",
    "print(\"Do not push data/cache/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
