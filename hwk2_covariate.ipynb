{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc54113f-3bee-41ad-bed2-faaf203e5382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MA_ROOT /scion/5261/econ470001/ma-data/ma\n",
      "LANDSCAPE_DIR /scion/5261/econ470001/ma-data/ma/landscape/Extracted Data\n",
      "FFSCOST_ROOT /scion/5261/econ470001/ma-data/ffs-costs\n",
      "PEN_DIR /scion/5261/econ470001/ma-data/ma/penetration/Extracted Data\n",
      "OUT_DIR /home/rpat638/econ470/a0/work/hwk2/data/processed\n",
      "Target years [2014, 2015, 2016, 2017, 2018, 2019]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 180)\n",
    "\n",
    "def pick_existing(paths: list[Path]) -> Path:\n",
    "    for p in paths:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(\"No candidate path exists\\n\" + \"\\n\".join(map(str, paths)))\n",
    "\n",
    "CANDIDATE_MA_ROOTS = [\n",
    "    Path(\"/scion/5261/econ470001/ma-data/ma\"),\n",
    "    Path(\"/econ470/a0/work/ma-data/ma\"),\n",
    "    Path.cwd().parent / \"ma-data\" / \"ma\",\n",
    "]\n",
    "\n",
    "MA_ROOT = pick_existing(CANDIDATE_MA_ROOTS)\n",
    "LANDSCAPE_DIR = pick_existing([MA_ROOT / \"landscape\" / \"Extracted Data\"])\n",
    "FFSCOST_ROOT = pick_existing([MA_ROOT.parent / \"ffs-costs\"])\n",
    "PEN_DIR = pick_existing([MA_ROOT / \"penetration\" / \"Extracted Data\"])\n",
    "\n",
    "# Standardize: write build outputs to processed for analysis\n",
    "OUT_DIR = Path(\"data/processed\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "YEARS = list(range(2014, 2020))\n",
    "\n",
    "print(\"MA_ROOT\", MA_ROOT)\n",
    "print(\"LANDSCAPE_DIR\", LANDSCAPE_DIR)\n",
    "print(\"FFSCOST_ROOT\", FFSCOST_ROOT)\n",
    "print(\"PEN_DIR\", PEN_DIR)\n",
    "print(\"OUT_DIR\", OUT_DIR.resolve())\n",
    "print(\"Target years\", YEARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab797942-3db7-48b7-b96e-256a5d7861aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_colname(s: str) -> str:\n",
    "    s = str(s).strip().lower()\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-z0-9_]\", \"\", s)\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s\n",
    "\n",
    "def parse_year_from_name(p: Path) -> int | None:\n",
    "    m = re.search(r\"(20\\d{2})\", p.name)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def detect_header_row_csv(path: Path, nrows: int = 100) -> int:\n",
    "    preview = pd.read_csv(\n",
    "        path,\n",
    "        header=None,\n",
    "        nrows=nrows,\n",
    "        dtype=str,\n",
    "        encoding_errors=\"replace\",\n",
    "        engine=\"python\",\n",
    "        on_bad_lines=\"skip\",\n",
    "    ).fillna(\"\").astype(str)\n",
    "\n",
    "    keys = [\n",
    "        \"state\", \"county\", \"contract\", \"contract id\",\n",
    "        \"plan\", \"plan id\", \"pbp\",\n",
    "        \"organization\", \"bid\", \"benchmark\", \"premium\",\n",
    "        \"code\", \"enrollment\", \"reimbursement\", \"per capita\",\n",
    "        \"fips\", \"ssa\", \"penetration\", \"eligibles\", \"enrolled\",\n",
    "    ]\n",
    "\n",
    "    best_i = 0\n",
    "    best_hits = -1\n",
    "    for i in range(preview.shape[0]):\n",
    "        row = \" \".join(preview.iloc[i].tolist()).lower()\n",
    "        hits = sum(1 for k in keys if k in row)\n",
    "        if hits > best_hits:\n",
    "            best_hits = hits\n",
    "            best_i = i\n",
    "    return best_i\n",
    "\n",
    "def read_csv_autoheader(path: Path) -> pd.DataFrame:\n",
    "    header_row = detect_header_row_csv(path)\n",
    "    df = pd.read_csv(\n",
    "        path,\n",
    "        skiprows=header_row,\n",
    "        header=0,\n",
    "        dtype=str,\n",
    "        encoding_errors=\"replace\",\n",
    "        engine=\"python\",\n",
    "        on_bad_lines=\"skip\",\n",
    "    )\n",
    "    df.columns = [norm_colname(c) for c in df.columns]\n",
    "    df = df.dropna(how=\"all\")\n",
    "    return df\n",
    "\n",
    "def detect_header_row_excel(path: Path, nrows: int = 80) -> int:\n",
    "    preview = pd.read_excel(path, header=None, nrows=nrows, engine=\"openpyxl\").fillna(\"\").astype(str)\n",
    "    keys = [\"code\", \"state\", \"county\", \"part a\", \"part b\", \"enrollment\", \"reimbursement\", \"per capita\"]\n",
    "    best_i = 0\n",
    "    best_hits = -1\n",
    "    for i in range(preview.shape[0]):\n",
    "        row = \" \".join(preview.iloc[i].tolist()).lower()\n",
    "        hits = sum(1 for k in keys if k in row)\n",
    "        if hits > best_hits:\n",
    "            best_hits = hits\n",
    "            best_i = i\n",
    "    return best_i\n",
    "\n",
    "def read_excel_autoheader(path: Path) -> pd.DataFrame:\n",
    "    header_row = detect_header_row_excel(path)\n",
    "    df = pd.read_excel(path, skiprows=header_row, header=0, engine=\"openpyxl\", dtype=str)\n",
    "    df.columns = [norm_colname(c) for c in df.columns]\n",
    "    df = df.dropna(how=\"all\")\n",
    "    return df\n",
    "\n",
    "def first_col(cols: list[str], needles: list[str]) -> str | None:\n",
    "    cols_l = [c.lower() for c in cols]\n",
    "    for n in needles:\n",
    "        for i, c in enumerate(cols_l):\n",
    "            if n in c:\n",
    "                return cols[i]\n",
    "    return None\n",
    "\n",
    "def digits5(s: pd.Series) -> pd.Series:\n",
    "    out = (\n",
    "        s.astype(str).str.strip()\n",
    "        .str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "        .str.replace(r\"[^0-9]\", \"\", regex=True)\n",
    "        .str.zfill(5)\n",
    "    )\n",
    "    out = out.where(out.str.len() == 5, np.nan)\n",
    "    return out\n",
    "\n",
    "def digits2(s: pd.Series) -> pd.Series:\n",
    "    out = (\n",
    "        s.astype(str).str.strip()\n",
    "        .str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "        .str.replace(r\"[^0-9]\", \"\", regex=True)\n",
    "        .str.zfill(2)\n",
    "    )\n",
    "    out = out.where(out.str.len() == 2, np.nan)\n",
    "    return out\n",
    "\n",
    "def digits3(s: pd.Series) -> pd.Series:\n",
    "    out = (\n",
    "        s.astype(str).str.strip()\n",
    "        .str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "        .str.replace(r\"[^0-9]\", \"\", regex=True)\n",
    "        .str.zfill(3)\n",
    "    )\n",
    "    out = out.where(out.str.len() == 3, np.nan)\n",
    "    return out\n",
    "\n",
    "def clean_contract_id(x: pd.Series) -> pd.Series:\n",
    "    s = x.astype(str).str.strip()\n",
    "    s = s.replace({\"nan\": np.nan, \"none\": np.nan, \"None\": np.nan, \"\": np.nan})\n",
    "    s = s.str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "    return s\n",
    "\n",
    "def clean_plan_id_3(x: pd.Series) -> pd.Series:\n",
    "    s = x.astype(str).str.strip()\n",
    "    s = s.replace({\"nan\": np.nan, \"none\": np.nan, \"None\": np.nan, \"\": np.nan})\n",
    "    s = s.str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "    s = s.str.replace(r\"[^0-9]\", \"\", regex=True)\n",
    "    s = s.where(s.str.len() > 0, np.nan)\n",
    "    return s.str.zfill(3)\n",
    "\n",
    "def to_num(s: pd.Series) -> pd.Series:\n",
    "    return pd.to_numeric(\n",
    "        s.astype(str)\n",
    "         .str.replace(\",\", \"\", regex=False)\n",
    "         .str.replace(\"$\", \"\", regex=False)\n",
    "         .str.replace(\"%\", \"\", regex=False)\n",
    "         .str.strip(),\n",
    "        errors=\"coerce\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "060f6201-d2ef-4fd7-9ee2-3fa1956e9774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 2014 2014LandscapeSource file MA_AtoM 05292014.csv shape (15444, 17)\n",
      "loaded 2014 2014LandscapeSource file MA_NtoW 05292014.csv shape (19536, 17)\n",
      "loaded 2014 508_AlabamatoMontana 05292014.csv shape (15855, 29)\n",
      "loaded 2014 508_NebraskatoWyoming 05292014.csv shape (20301, 29)\n",
      "loaded 2015 2015LandscapeSource file MA_AtoM 11042014.csv shape (15881, 17)\n",
      "loaded 2015 2015LandscapeSource file MA_NtoW 11042014.csv shape (17695, 17)\n",
      "loaded 2015 508_AlabamatoMontana 03182015.csv shape (16662, 28)\n",
      "loaded 2015 508_NebraskatoWyoming 03182015.csv shape (5675, 28)\n",
      "loaded 2016 2016LandscapeSource file MA_AtoM 04222016.csv shape (16096, 18)\n",
      "loaded 2016 2016LandscapeSource file MA_NtoW 04222016.csv shape (18122, 18)\n",
      "loaded 2016 508_AlabamatoMontana 04222016.csv shape (17046, 28)\n",
      "loaded 2016 508_NebraskatoWyoming 04222016.csv shape (20396, 28)\n",
      "loaded 2016 508_AlabamatoMontana 10182016.csv shape (17653, 30)\n",
      "loaded 2016 508_NebraskatoWyoming 10182016.csv shape (20213, 28)\n",
      "loaded 2017 2017LandscapeSource file MA_AtoM 10182016.csv shape (16827, 18)\n",
      "loaded 2017 2017LandscapeSource file MA_NtoW 10182016.csv shape (17675, 18)\n",
      "loaded 2017 508_AlabamatoMontana 09222017.csv shape (19371, 29)\n",
      "loaded 2017 508_NebraskatoWyoming 09222017.csv shape (23055, 28)\n",
      "loaded 2018 2018LandscapeSource file MA_AtoM 10142017.csv shape (18289, 18)\n",
      "loaded 2018 2018LandscapeSource file MA_NtoW 10142017.csv shape (19820, 18)\n",
      "loaded 2018 508_AlabamatoMontana 10012018.csv shape (22229, 28)\n",
      "loaded 2018 508_NebraskatoWyoming 10012018.csv shape (27120, 28)\n",
      "loaded 2019 2019LandscapeSource file MA_AtoM 10122018.csv shape (20395, 18)\n",
      "loaded 2019 2019LandscapeSource file MA_NtoW 10122018.csv shape (23404, 18)\n",
      "loaded 2019 508_AlabamatoMontana 09032019.csv shape (27047, 30)\n",
      "loaded 2019 508_NebraskatoWyoming 09032019.csv shape (31002, 28)\n",
      "landscape_clean shape (502809, 10)\n",
      "Wrote data/processed/landscape_clean_2014_2019.csv\n"
     ]
    }
   ],
   "source": [
    "def landscape_files_for_year(root: Path, year: int) -> list[Path]:\n",
    "    files = sorted([p for p in root.rglob(\"*.csv\") if p.is_file()])\n",
    "    hits = []\n",
    "    for p in files:\n",
    "        y = parse_year_from_name(p)\n",
    "        if y != year:\n",
    "            continue\n",
    "        n = p.name.lower()\n",
    "        if \"sanction\" in n or \"importantnotes\" in n or \"partd\" in n or \"part_d\" in n or \"premium\" in n:\n",
    "            continue\n",
    "        hits.append(p)\n",
    "    return hits\n",
    "\n",
    "def build_landscape_clean(years: list[int]) -> pd.DataFrame:\n",
    "    out = []\n",
    "    for y in years:\n",
    "        files = landscape_files_for_year(LANDSCAPE_DIR, y)\n",
    "        if not files:\n",
    "            print(\"No landscape CSVs found for\", y)\n",
    "            continue\n",
    "\n",
    "        chunks = []\n",
    "        for p in files:\n",
    "            df = read_csv_autoheader(p)\n",
    "            df[\"year\"] = y\n",
    "            df[\"source_file\"] = p.name\n",
    "            chunks.append(df)\n",
    "            print(\"loaded\", y, p.name, \"shape\", df.shape)\n",
    "\n",
    "        d = pd.concat(chunks, ignore_index=True)\n",
    "        cols = list(d.columns)\n",
    "\n",
    "        contract_col = first_col(cols, [\"contract_id\", \"contractid\", \"contract\"])\n",
    "        planid_col = first_col(cols, [\"plan_id\", \"planid\", \"pbp\"])\n",
    "        if planid_col is None:\n",
    "            planid_col = first_col(cols, [\"plan\"])\n",
    "        state_col = first_col(cols, [\"state\"])\n",
    "        county_col = first_col(cols, [\"county\"])\n",
    "        bid_col = first_col(cols, [\"bid\"])\n",
    "        bmk_col = first_col(cols, [\"benchmark\"])\n",
    "        org_col = first_col(cols, [\"organization\", \"org\"])\n",
    "        pname_col = first_col(cols, [\"plan_name\", \"planname\"])\n",
    "\n",
    "        if contract_col is None or planid_col is None:\n",
    "            print(\"Year\", y, \"missing contract or plan columns\")\n",
    "            print(\"First 80 columns\", cols[:80])\n",
    "            continue\n",
    "\n",
    "        dd = pd.DataFrame()\n",
    "        dd[\"contract_id\"] = clean_contract_id(d[contract_col])\n",
    "        dd[\"plan_id\"] = clean_plan_id_3(d[planid_col])\n",
    "        dd[\"state\"] = d[state_col] if state_col is not None else np.nan\n",
    "        dd[\"county\"] = d[county_col] if county_col is not None else np.nan\n",
    "        dd[\"organization_name\"] = d[org_col] if org_col is not None else np.nan\n",
    "        dd[\"plan_name\"] = d[pname_col] if pname_col is not None else np.nan\n",
    "        dd[\"bid\"] = to_num(d[bid_col]) if bid_col is not None else np.nan\n",
    "        dd[\"benchmark\"] = to_num(d[bmk_col]) if bmk_col is not None else np.nan\n",
    "        dd[\"year\"] = y\n",
    "        dd[\"source_file\"] = d[\"source_file\"]\n",
    "\n",
    "        dd = dd.dropna(subset=[\"contract_id\", \"plan_id\"]).copy()\n",
    "        out.append(dd)\n",
    "\n",
    "    if not out:\n",
    "        raise RuntimeError(\"No landscape data loaded\")\n",
    "    return pd.concat(out, ignore_index=True)\n",
    "\n",
    "landscape_clean = build_landscape_clean(YEARS)\n",
    "print(\"landscape_clean shape\", landscape_clean.shape)\n",
    "\n",
    "landscape_out = OUT_DIR / \"landscape_clean_2014_2019.csv\"\n",
    "landscape_clean.to_csv(landscape_out, index=False)\n",
    "print(\"Wrote\", landscape_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adab1229-fb8f-47d2-950e-fa88a58b35d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openpyxl available\n",
      "Pen crosswalk 2014 rows 3222 from State_County_Penetration_MA_2014_01.csv\n",
      "Pen crosswalk 2015 rows 3222 from State_County_Penetration_MA_2015_01.csv\n",
      "Pen crosswalk 2016 rows 3141 from State_County_Penetration_MA_2016_01.csv\n",
      "Pen crosswalk 2017 rows 3219 from State_County_Penetration_MA_2017_01.csv\n",
      "Pen crosswalk 2018 rows 3219 from State_County_Penetration_MA_2018_01.csv\n",
      "Pen crosswalk 2019 rows 3215 from State_County_Penetration_MA_2019_01.csv\n",
      "pen_cw_all shape (19238, 3)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import openpyxl  # noqa\n",
    "    print(\"openpyxl available\")\n",
    "except Exception:\n",
    "    print(\"openpyxl missing, installing\")\n",
    "    !{sys.executable} -m pip install --user openpyxl\n",
    "\n",
    "def penetration_files_for_year(year: int) -> list[Path]:\n",
    "    files = [p for p in PEN_DIR.rglob(\"*\") if p.is_file() and p.suffix.lower() in [\".csv\", \".txt\"]]\n",
    "    hits = [p for p in files if re.search(rf\"{year}\", p.name)]\n",
    "    hits = sorted(hits, key=lambda p: (len(p.name), p.name.lower()))\n",
    "    return hits\n",
    "\n",
    "def load_penetration_crosswalk(year: int) -> pd.DataFrame:\n",
    "    cands = penetration_files_for_year(year)\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(f\"No penetration files found for {year} under {PEN_DIR}\")\n",
    "\n",
    "    pick = None\n",
    "    for p in cands:\n",
    "        if re.search(r\"state_county_penetration\", p.name.lower()):\n",
    "            pick = p\n",
    "            break\n",
    "    if pick is None:\n",
    "        pick = cands[0]\n",
    "\n",
    "    df = read_csv_autoheader(pick)\n",
    "    cols = list(df.columns)\n",
    "\n",
    "    # county fips\n",
    "    if \"fips\" in df.columns:\n",
    "        fips5 = digits5(df[\"fips\"])\n",
    "    else:\n",
    "        fipsst = first_col(cols, [\"fipsst\"])\n",
    "        fipscnty = first_col(cols, [\"fipscnty\"])\n",
    "        if fipsst is None or fipscnty is None:\n",
    "            raise KeyError(f\"Penetration file missing fips columns: {pick.name}\")\n",
    "        fips5 = digits2(df[fipsst]) + digits3(df[fipscnty])\n",
    "\n",
    "    # ssa code\n",
    "    if \"ssa\" in df.columns:\n",
    "        ssa5 = digits5(df[\"ssa\"])\n",
    "    else:\n",
    "        ssast = first_col(cols, [\"ssast\"])\n",
    "        ssacnty = first_col(cols, [\"ssacnty\"])\n",
    "        if ssast is None or ssacnty is None:\n",
    "            raise KeyError(f\"Penetration file missing ssa columns: {pick.name}\")\n",
    "        ssa5 = digits2(df[ssast]) + digits3(df[ssacnty])\n",
    "\n",
    "    out = pd.DataFrame({\"year\": int(year), \"ssa\": ssa5, \"fips\": fips5})\n",
    "    out = out.dropna(subset=[\"ssa\", \"fips\"]).drop_duplicates(subset=[\"year\", \"ssa\", \"fips\"]).copy()\n",
    "\n",
    "    # drop junk totals if present\n",
    "    out = out[(out[\"ssa\"] != \"00000\") & (out[\"fips\"] != \"00000\")].copy()\n",
    "\n",
    "    print(\"Pen crosswalk\", year, \"rows\", len(out), \"from\", pick.name)\n",
    "    return out\n",
    "\n",
    "pen_cw_all = pd.concat([load_penetration_crosswalk(y) for y in YEARS], ignore_index=True)\n",
    "print(\"pen_cw_all shape\", pen_cw_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32fd715a-f07e-4846-98dd-323a3e6ed5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MASTER_PATH exists True /scion/5261/econ470001/ma-data/ffs-costs/CMS FFS Costs.csv\n",
      "Loaded 2016 from /scion/5261/econ470001/ma-data/ffs-costs/Extracted Data/FFS16.xlsx rows 3126\n",
      "Loaded 2017 from /scion/5261/econ470001/ma-data/ffs-costs/Extracted Data/ffs2017/FFS17.xlsx rows 3205\n",
      "Loaded 2018 from /scion/5261/econ470001/ma-data/ffs-costs/Extracted Data/FFS2018/FFS18.xlsx rows 3205\n",
      "Loaded 2019 from /scion/5261/econ470001/ma-data/ffs-costs/Extracted Data/FFS2019/FFS19.xlsx rows 3203\n",
      "Wrote data/processed/ffs_cost_2014_2019.csv rows 19147\n"
     ]
    }
   ],
   "source": [
    "MASTER_PATH = FFSCOST_ROOT / \"CMS FFS Costs.csv\"\n",
    "print(\"MASTER_PATH exists\", MASTER_PATH.exists(), MASTER_PATH)\n",
    "\n",
    "def compute_ffs_cost(df: pd.DataFrame) -> pd.Series:\n",
    "    cols = list(df.columns)\n",
    "\n",
    "    a_en_col = first_col(cols, [\"part_a_enrollment\"])\n",
    "    b_en_col = first_col(cols, [\"part_b_enrollment\"])\n",
    "    a_rb_col = first_col(cols, [\"part_a_total_reimbursement\"])\n",
    "    b_rb_col = first_col(cols, [\"part_b_total_reimbursement\"])\n",
    "\n",
    "    if a_en_col is None or b_en_col is None or a_rb_col is None or b_rb_col is None:\n",
    "        raise KeyError(\"FFS file missing A/B enrollment or reimbursement columns\")\n",
    "\n",
    "    denom = (to_num(df[a_en_col]) + to_num(df[b_en_col])).replace(0, np.nan)\n",
    "    return (to_num(df[a_rb_col]) + to_num(df[b_rb_col])) / denom\n",
    "\n",
    "def load_ffs_from_master(master_path: Path) -> pd.DataFrame:\n",
    "    df = read_csv_autoheader(master_path)\n",
    "    cols = list(df.columns)\n",
    "\n",
    "    code_col = first_col(cols, [\"code\"])\n",
    "    year_col = first_col(cols, [\"year\"])\n",
    "\n",
    "    if code_col is None or year_col is None:\n",
    "        raise KeyError(\"FFS master missing code or year\")\n",
    "\n",
    "    tmp = df.copy()\n",
    "    tmp[\"year\"] = pd.to_numeric(tmp[year_col], errors=\"coerce\").astype(\"Int64\")\n",
    "    tmp[\"ssa\"] = digits5(tmp[code_col])\n",
    "    tmp = tmp.dropna(subset=[\"year\", \"ssa\"]).copy()\n",
    "    tmp[\"year\"] = tmp[\"year\"].astype(int)\n",
    "\n",
    "    out_all = []\n",
    "    for y in sorted(tmp[\"year\"].unique().tolist()):\n",
    "        if y not in YEARS:\n",
    "            continue\n",
    "        cw = pen_cw_all[pen_cw_all[\"year\"] == y][[\"ssa\", \"fips\"]].copy()\n",
    "        m = tmp[tmp[\"year\"] == y].merge(cw, on=\"ssa\", how=\"left\")\n",
    "        m[\"ffs_cost\"] = compute_ffs_cost(m)\n",
    "        out = m[[\"fips\", \"year\", \"ffs_cost\"]].dropna(subset=[\"fips\", \"ffs_cost\"]).copy()\n",
    "        out_all.append(out)\n",
    "\n",
    "    out = pd.concat(out_all, ignore_index=True).drop_duplicates(subset=[\"fips\", \"year\"])\n",
    "    return out\n",
    "\n",
    "def find_ffs_excel(ffscost_root: Path, year: int) -> Path | None:\n",
    "    extracted = ffscost_root / \"Extracted Data\"\n",
    "    yy = str(year)[-2:]\n",
    "    pats = [f\"*FFS{yy}*.xlsx\", f\"*ffs{yy}*.xlsx\", f\"*FFS{year}*.xlsx\", f\"*ffs{year}*.xlsx\"]\n",
    "    hits = []\n",
    "    for pat in pats:\n",
    "        hits.extend(list(extracted.rglob(pat)))\n",
    "    hits = [p for p in hits if p.is_file() and \"~$\" not in p.name]\n",
    "    if not hits:\n",
    "        return None\n",
    "    hits = sorted(hits, key=lambda p: (len(str(p)), str(p).lower()))\n",
    "    return hits[0]\n",
    "\n",
    "def load_ffs_from_excel_year(ffscost_root: Path, year: int) -> pd.DataFrame:\n",
    "    xlsx_path = find_ffs_excel(ffscost_root, year)\n",
    "    if xlsx_path is None:\n",
    "        raise FileNotFoundError(f\"Missing Excel for {year} under {ffscost_root}/Extracted Data\")\n",
    "\n",
    "    df = read_excel_autoheader(xlsx_path)\n",
    "    cols = list(df.columns)\n",
    "\n",
    "    code_col = first_col(cols, [\"code\"])\n",
    "    if code_col is None:\n",
    "        raise KeyError(f\"Excel missing code column in {xlsx_path.name}\")\n",
    "\n",
    "    tmp = df.copy()\n",
    "    tmp[\"ssa\"] = digits5(tmp[code_col])\n",
    "    tmp[\"year\"] = int(year)\n",
    "\n",
    "    cw = pen_cw_all[pen_cw_all[\"year\"] == year][[\"ssa\", \"fips\"]].copy()\n",
    "    m = tmp.merge(cw, on=\"ssa\", how=\"left\")\n",
    "\n",
    "    m[\"ffs_cost\"] = compute_ffs_cost(m)\n",
    "    out = m[[\"fips\", \"year\", \"ffs_cost\"]].dropna(subset=[\"fips\", \"ffs_cost\"]).copy()\n",
    "\n",
    "    print(\"Loaded\", year, \"from\", xlsx_path, \"rows\", len(out))\n",
    "    return out\n",
    "\n",
    "pieces = []\n",
    "if MASTER_PATH.exists():\n",
    "    pieces.append(load_ffs_from_master(MASTER_PATH))\n",
    "\n",
    "for y in [2016, 2017, 2018, 2019]:\n",
    "    pieces.append(load_ffs_from_excel_year(FFSCOST_ROOT, y))\n",
    "\n",
    "ffs_cost = pd.concat(pieces, ignore_index=True).drop_duplicates(subset=[\"fips\", \"year\"]).copy()\n",
    "ffs_cost = ffs_cost[ffs_cost[\"year\"].isin(YEARS)].copy()\n",
    "\n",
    "ffs_out = OUT_DIR / \"ffs_cost_2014_2019.csv\"\n",
    "ffs_cost.to_csv(ffs_out, index=False)\n",
    "print(\"Wrote\", ffs_out, \"rows\", len(ffs_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67cc2d5b-cd9f-4a7a-bb7f-fa12fc9500b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote data/processed/ffs_cost_2018_with_quartiles.csv rows 3205\n",
      "Post merge quartile missing rate: 0.006821705426356589\n",
      "Merged rows: (3225, 10)\n"
     ]
    }
   ],
   "source": [
    "def quartile_codes(x: pd.Series) -> pd.Series:\n",
    "    x = pd.to_numeric(x, errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "    cats = pd.qcut(x, 4, duplicates=\"drop\")\n",
    "    codes = cats.cat.codes.replace(-1, np.nan) + 1\n",
    "    return codes.astype(\"Int64\")\n",
    "\n",
    "ffs18 = ffs_cost[ffs_cost[\"year\"] == 2018].copy()\n",
    "ffs18[\"ffs_quartile\"] = quartile_codes(ffs18[\"ffs_cost\"])\n",
    "\n",
    "ffs18_out = OUT_DIR / \"ffs_cost_2018_with_quartiles.csv\"\n",
    "ffs18[[\"fips\", \"year\", \"ffs_cost\", \"ffs_quartile\"]].to_csv(ffs18_out, index=False)\n",
    "print(\"Wrote\", ffs18_out, \"rows\", len(ffs18))\n",
    "\n",
    "# Optional: quick merge check if you already built county_bid_hhi_2018\n",
    "county_path = OUT_DIR / \"county_bid_hhi_2018.csv\"\n",
    "if county_path.exists():\n",
    "    county = pd.read_csv(county_path, dtype={\"fips\": str}, low_memory=False)\n",
    "    county[\"fips\"] = county[\"fips\"].astype(str).str.zfill(5)\n",
    "    county[\"year\"] = pd.to_numeric(county[\"year\"], errors=\"coerce\").fillna(2018).astype(int)\n",
    "\n",
    "    test = county.merge(ffs18[[\"fips\", \"year\", \"ffs_quartile\"]], on=[\"fips\", \"year\"], how=\"left\")\n",
    "    print(\"Post merge quartile missing rate:\", test[\"ffs_quartile\"].isna().mean())\n",
    "    print(\"Merged rows:\", test.shape)\n",
    "else:\n",
    "    print(\"Skipping county merge check; missing\", county_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
