{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "836c76f3-94ce-4c3f-b1ba-cc7841e2475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 140)\n",
    "\n",
    "YEARS = list(range(2014, 2020))\n",
    "\n",
    "OUT_DIR = Path(\"data/processed\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CANDIDATE_MA_ROOTS = [\n",
    "    Path(\"/scion/5261/econ470001/ma-data/ma\"),\n",
    "    Path(\"/econ470/a0/work/ma-data/ma\"),\n",
    "    Path.cwd().parent / \"ma-data\" / \"ma\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f55731bb-f505-417a-854d-b80954fa495b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MA_ROOT /scion/5261/econ470001/ma-data/ma\n",
      "ENROLL_EXTRACTED exists True\n",
      "SAREA_EXTRACTED exists True\n",
      "PEN_EXTRACTED exists True\n"
     ]
    }
   ],
   "source": [
    "def pick_existing(paths: list[Path]) -> Path | None:\n",
    "    for p in paths:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "MA_ROOT = pick_existing(CANDIDATE_MA_ROOTS)\n",
    "if MA_ROOT is None:\n",
    "    raise FileNotFoundError(\"Could not find ma-data/ma. Update CANDIDATE_MA_ROOTS if needed.\")\n",
    "\n",
    "ENROLL_EXTRACTED  = MA_ROOT / \"enrollment\" / \"Extracted Data\"\n",
    "SAREA_EXTRACTED   = MA_ROOT / \"service-area\" / \"Extracted Data\"\n",
    "PEN_EXTRACTED     = MA_ROOT / \"penetration\" / \"Extracted Data\"\n",
    "\n",
    "print(\"MA_ROOT\", MA_ROOT)\n",
    "print(\"ENROLL_EXTRACTED exists\", ENROLL_EXTRACTED.exists())\n",
    "print(\"SAREA_EXTRACTED exists\", SAREA_EXTRACTED.exists())\n",
    "print(\"PEN_EXTRACTED exists\", PEN_EXTRACTED.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60b2a3ed-5517-454c-a29d-7f06d8485492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_colname(s: str) -> str:\n",
    "    s = s.strip().lower()\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-z0-9_]\", \"\", s)\n",
    "    return s\n",
    "\n",
    "def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [norm_colname(c) for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def first_existing_col(cols: list[str], candidates: list[str]) -> str | None:\n",
    "    s = set(cols)\n",
    "    for c in candidates:\n",
    "        if c in s:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def coerce_id_series(x: pd.Series, width: int | None = None) -> pd.Series:\n",
    "    x = x.astype(str).str.strip()\n",
    "    x = x.replace({\"nan\": np.nan, \"None\": np.nan, \"\": np.nan})\n",
    "    x = x.str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "    if width is not None:\n",
    "        x = x.str.zfill(width)\n",
    "    return x\n",
    "\n",
    "def to_numeric_clean(x: pd.Series) -> pd.Series:\n",
    "    s = x.astype(str).str.replace(\",\", \"\", regex=False).str.strip()\n",
    "    s = s.replace({\"nan\": np.nan, \"None\": np.nan, \"\": np.nan, \"*\": np.nan})\n",
    "    return pd.to_numeric(s, errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "357cd3d2-87a3-4d07-a0b2-c8be4b2a3b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _digits_zfill(x: pd.Series, width: int) -> pd.Series:\n",
    "    s = x.astype(str).str.strip()\n",
    "    s = s.replace({\"nan\": np.nan, \"None\": np.nan, \"\": np.nan})\n",
    "    s = s.str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "    s = s.str.replace(r\"[^0-9]\", \"\", regex=True)\n",
    "    s = s.where(s.str.len() > 0, np.nan)\n",
    "    return s.str.zfill(width)\n",
    "\n",
    "def coerce_fips(df: pd.DataFrame) -> pd.Series:\n",
    "    cols = list(df.columns)\n",
    "\n",
    "    fips_col = first_existing_col(\n",
    "        cols,\n",
    "        [\n",
    "            \"fips\",\n",
    "            \"fips_state_county_code\",   # your enrollment file\n",
    "            \"fipsstatecountycode\",\n",
    "            \"county_fips\",\n",
    "            \"cnty_fips\",\n",
    "            \"county_fips_code\",\n",
    "            \"fipscounty\",\n",
    "            \"fipscnty\",\n",
    "        ],\n",
    "    )\n",
    "    if fips_col is not None:\n",
    "        return _digits_zfill(df[fips_col], 5)\n",
    "\n",
    "    st_col = first_existing_col(\n",
    "        cols,\n",
    "        [\"state_fips\", \"statefips\", \"fipsst\", \"state_fipscode\", \"state_code\"]\n",
    "    )\n",
    "    ct_col = first_existing_col(\n",
    "        cols,\n",
    "        [\"county_fips\", \"countyfips\", \"fipscnty\", \"county_code\", \"cnty\"]\n",
    "    )\n",
    "\n",
    "    if st_col is not None and ct_col is not None:\n",
    "        st = _digits_zfill(df[st_col], 2)\n",
    "        ct = _digits_zfill(df[ct_col], 3)\n",
    "        out = (st.fillna(\"\") + ct.fillna(\"\")).replace({\"\": np.nan})\n",
    "        return out\n",
    "\n",
    "    return pd.Series([np.nan] * len(df), index=df.index, dtype=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d968ee9a-f6b9-4e0d-8685-dcaac0bb0a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_year_file(root: Path, year: int, kind: str) -> Path:\n",
    "    files = [p for p in root.rglob(\"*\") if p.is_file()]\n",
    "\n",
    "    if kind == \"penetration\":\n",
    "        pats = [rf\".*{year}.*penet.*\\.(csv|txt)$\", rf\".*penet.*{year}.*\\.(csv|txt)$\"]\n",
    "    else:\n",
    "        raise ValueError(\"unknown kind\")\n",
    "\n",
    "    for pat in pats:\n",
    "        rx = re.compile(pat, flags=re.I)\n",
    "        hits = [p for p in files if rx.match(p.name)]\n",
    "        if hits:\n",
    "            hits.sort(key=lambda p: len(p.name))\n",
    "            return hits[0]\n",
    "\n",
    "    rx = re.compile(rf\".*{year}.*\\.(csv|txt)$\", flags=re.I)\n",
    "    hits = [p for p in files if rx.match(p.name)]\n",
    "    if hits:\n",
    "        hits.sort(key=lambda p: len(p.name))\n",
    "        return hits[0]\n",
    "\n",
    "    raise FileNotFoundError(f\"Could not find {kind} file for {year} under {root}\")\n",
    "\n",
    "def find_month_file(root: Path, year: int, month: int, kind: str) -> Path:\n",
    "    m2 = f\"{month:02d}\"\n",
    "    files = [p for p in root.rglob(\"*\") if p.is_file()]\n",
    "\n",
    "    if kind == \"enroll\":\n",
    "        pats = [rf\".*{year}.*{m2}.*enroll.*\\.(csv|txt)$\", rf\".*enroll.*{year}.*{m2}.*\\.(csv|txt)$\"]\n",
    "    elif kind == \"sarea\":\n",
    "        pats = [rf\".*{year}.*{m2}.*(service|sa|sarea).*?\\.(csv|txt)$\", rf\".*(service|sa|sarea).*{year}.*{m2}.*?\\.(csv|txt)$\"]\n",
    "    else:\n",
    "        raise ValueError(\"unknown kind\")\n",
    "\n",
    "    for pat in pats:\n",
    "        rx = re.compile(pat, flags=re.I)\n",
    "        hits = [p for p in files if rx.match(p.name)]\n",
    "        if hits:\n",
    "            hits.sort(key=lambda p: len(p.name))\n",
    "            return hits[0]\n",
    "\n",
    "    rx = re.compile(rf\".*{year}.*{m2}.*\\.(csv|txt)$\", flags=re.I)\n",
    "    hits = [p for p in files if rx.match(p.name)]\n",
    "    if hits:\n",
    "        hits.sort(key=lambda p: len(p.name))\n",
    "        return hits[0]\n",
    "\n",
    "    raise FileNotFoundError(f\"Could not find {kind} file for {year}-{m2} under {root}\")\n",
    "\n",
    "def read_csv_any(path: Path) -> pd.DataFrame:\n",
    "    return pd.read_csv(\n",
    "        path,\n",
    "        dtype=str,\n",
    "        na_values=[\"*\", \"NA\", \"N/A\", \"\"],\n",
    "        keep_default_na=True,\n",
    "        encoding_errors=\"replace\",\n",
    "        low_memory=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dc06ba7-1157-4aca-bd57-ad4e4ba7082b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_keys(df: pd.DataFrame, require_plan_id: bool = True) -> pd.DataFrame:\n",
    "    df = normalize_columns(df)\n",
    "    cols = list(df.columns)\n",
    "\n",
    "    contract_col = first_existing_col(cols, [\"contract_id\", \"contractid\", \"contr_id\", \"contract\", \"contract_number\"])\n",
    "    plan_col     = first_existing_col(cols, [\"plan_id\", \"planid\", \"plan\", \"plan_number\", \"pln_id\"])\n",
    "\n",
    "    if contract_col is None:\n",
    "        raise KeyError(f\"Missing contract id column. Sample columns {sorted(cols)[:40]}\")\n",
    "\n",
    "    out = df.copy()\n",
    "    out[\"contract_id\"] = coerce_id_series(out[contract_col], None)\n",
    "\n",
    "    if require_plan_id:\n",
    "        if plan_col is None:\n",
    "            raise KeyError(f\"Missing plan id column. Sample columns {sorted(cols)[:40]}\")\n",
    "        out[\"plan_id\"] = coerce_id_series(out[plan_col], 3)\n",
    "    else:\n",
    "        if plan_col is None:\n",
    "            out[\"plan_id\"] = np.nan\n",
    "        else:\n",
    "            out[\"plan_id\"] = coerce_id_series(out[plan_col], 3)\n",
    "\n",
    "    out[\"fips\"] = coerce_fips(out)\n",
    "    return out\n",
    "\n",
    "def pick_enrollment_col(df: pd.DataFrame) -> str:\n",
    "    cols = list(df.columns)\n",
    "    c = first_existing_col(\n",
    "        cols,\n",
    "        [\"enrollment\",\"enroll\",\"total_enrollment\",\"tot_enrollment\",\"enrollment_cnt\",\"enrollment_count\",\"plan_enrollment\"],\n",
    "    )\n",
    "    if c is None:\n",
    "        raise KeyError(f\"Missing enrollment column. Sample columns {sorted(cols)[:60]}\")\n",
    "    return c\n",
    "\n",
    "def pick_plan_meta_cols(df: pd.DataFrame) -> list[str]:\n",
    "    cols = list(df.columns)\n",
    "    candidates = [\n",
    "        \"plan_type\",\"plantype\",\"contract_type\",\"contracttype\",\"organization_type\",\"org_type\",\n",
    "        \"snp\",\"snp_type\",\"segment\",\"partd\",\"part_d\",\"partc\",\"part_c\",\"plan_name\",\n",
    "    ]\n",
    "    return [c for c in candidates if c in cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68c8cd8a-00f5-4274-9665-a8e52a5bf52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_exclusions(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    cols = list(df.columns)\n",
    "\n",
    "    df[\"is_800_series\"] = df[\"plan_id\"].astype(str).str.strip().str.startswith(\"8\")\n",
    "\n",
    "    snp_col = first_existing_col(cols, [\"snp\", \"snp_type\"])\n",
    "    if snp_col is None:\n",
    "        df[\"is_snp\"] = False\n",
    "    else:\n",
    "        s = df[snp_col].astype(str).str.lower().str.strip()\n",
    "        df[\"is_snp\"] = s.isin([\"y\", \"yes\", \"1\", \"true\"]) | (s.notna() & (s != \"\") & ~s.isin([\"n\", \"no\", \"0\", \"false\", \"nan\", \"none\"]))\n",
    "\n",
    "    partc_col = first_existing_col(cols, [\"partc\", \"part_c\"])\n",
    "    if partc_col is not None:\n",
    "        s = df[partc_col].astype(str).str.lower().str.strip()\n",
    "        df[\"is_pdp_only\"] = s.isin([\"n\", \"no\", \"0\", \"false\"])\n",
    "    else:\n",
    "        plan_type_col = first_existing_col(cols, [\"plan_type\", \"plantype\", \"contract_type\", \"contracttype\"])\n",
    "        if plan_type_col is None:\n",
    "            df[\"is_pdp_only\"] = False\n",
    "        else:\n",
    "            s = df[plan_type_col].astype(str).str.lower().str.strip()\n",
    "            df[\"is_pdp_only\"] = s.str.contains(r\"\\bpdp\\b\", regex=True) | s.str.contains(\"prescription drug\", regex=False)\n",
    "\n",
    "    df[\"drop_hw2\"] = df[\"is_snp\"] | df[\"is_800_series\"] | df[\"is_pdp_only\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "777a5eb6-e19c-4941-8afd-4d06a334569d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enroll fips non-missing share: 0.9972787896000975\n",
      "sarea  fips non-missing share: 0.9998615883637939\n",
      "  contract_id plan_id fips\n",
      "0       E0654     801  NaN\n",
      "1       E0654     801  NaN\n",
      "2       E0654     801  NaN\n",
      "  contract_id  plan_id   fips\n",
      "0       90091      NaN    NaN\n",
      "1       H0022      NaN  39023\n",
      "2       H0022      NaN  39035\n"
     ]
    }
   ],
   "source": [
    "year = 2018\n",
    "month = 1\n",
    "enroll_path = find_month_file(ENROLL_EXTRACTED, year, month, \"enroll\")\n",
    "sarea_path  = find_month_file(SAREA_EXTRACTED,  year, month, \"sarea\")\n",
    "\n",
    "en = standardize_keys(read_csv_any(enroll_path), require_plan_id=True)\n",
    "sa = standardize_keys(read_csv_any(sarea_path), require_plan_id=False)\n",
    "\n",
    "print(\"enroll fips non-missing share:\", en[\"fips\"].notna().mean())\n",
    "print(\"sarea  fips non-missing share:\", sa[\"fips\"].notna().mean())\n",
    "print(en[[\"contract_id\",\"plan_id\",\"fips\"]].head(3))\n",
    "print(sa[[\"contract_id\",\"plan_id\",\"fips\"]].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adbba3c2-199a-4de6-b9cc-87300689ccd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted data/processed/plan_county_year_2014_2019.csv\n",
      "deleted data/processed/county_plan_counts_2014_2019.csv\n",
      "deleted data/processed/county_hhi_ma_share_2014_2019.csv\n",
      "deleted data/processed/plan_county_year_2014.csv\n",
      "deleted data/processed/plan_county_year_2015.csv\n",
      "deleted data/processed/plan_county_year_2016.csv\n",
      "deleted data/processed/plan_county_year_2017.csv\n",
      "deleted data/processed/plan_county_year_2018.csv\n",
      "deleted data/processed/plan_county_year_2019.csv\n"
     ]
    }
   ],
   "source": [
    "paths_to_delete = [\n",
    "    OUT_DIR / \"plan_county_year_2014_2019.csv\",\n",
    "    OUT_DIR / \"county_plan_counts_2014_2019.csv\",\n",
    "    OUT_DIR / \"county_hhi_ma_share_2014_2019.csv\",\n",
    "]\n",
    "for p in paths_to_delete:\n",
    "    if p.exists():\n",
    "        p.unlink()\n",
    "        print(\"deleted\", p)\n",
    "\n",
    "for y in YEARS:\n",
    "    yp = OUT_DIR / f\"plan_county_year_{y}.csv\"\n",
    "    if yp.exists():\n",
    "        yp.unlink()\n",
    "        print(\"deleted\", yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57c6b17f-4993-4115-9603-58fd25b12dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_plan_county_year_one_year(year: int) -> Path:\n",
    "    out_path = OUT_DIR / f\"plan_county_year_{year}.csv\"\n",
    "    if out_path.exists():\n",
    "        return out_path\n",
    "\n",
    "    accum = None\n",
    "\n",
    "    for month in range(1, 13):\n",
    "        enroll_path = find_month_file(ENROLL_EXTRACTED, year, month, \"enroll\")\n",
    "        sarea_path  = find_month_file(SAREA_EXTRACTED,  year, month, \"sarea\")\n",
    "\n",
    "        enroll_raw = read_csv_any(enroll_path)\n",
    "        sarea_raw  = read_csv_any(sarea_path)\n",
    "\n",
    "        enroll = standardize_keys(enroll_raw, require_plan_id=True)\n",
    "        sarea  = standardize_keys(sarea_raw, require_plan_id=False)\n",
    "\n",
    "        enroll_col = pick_enrollment_col(enroll)\n",
    "        meta_cols  = pick_plan_meta_cols(enroll)\n",
    "\n",
    "        keep_enroll = [\"contract_id\", \"plan_id\", \"fips\", enroll_col] + meta_cols\n",
    "        keep_enroll = [c for c in keep_enroll if c in enroll.columns]\n",
    "        enroll = enroll[keep_enroll].copy()\n",
    "        enroll = enroll.rename(columns={enroll_col: \"enrollment\"})\n",
    "        enroll[\"enrollment\"] = to_numeric_clean(enroll[\"enrollment\"])\n",
    "\n",
    "        sarea_keys = [\"contract_id\", \"fips\"]\n",
    "        has_plan_in_sarea = sarea[\"plan_id\"].notna().any()\n",
    "        if has_plan_in_sarea:\n",
    "            sarea_keys = [\"contract_id\", \"plan_id\", \"fips\"]\n",
    "        sarea = sarea[sarea_keys].copy()\n",
    "\n",
    "        enroll = enroll.dropna(subset=[\"contract_id\", \"plan_id\", \"fips\"])\n",
    "        sarea  = sarea.dropna(subset=[\"contract_id\", \"fips\"])\n",
    "        if has_plan_in_sarea:\n",
    "            sarea = sarea.dropna(subset=[\"plan_id\"])\n",
    "\n",
    "        merge_keys = [\"contract_id\", \"fips\"] if not has_plan_in_sarea else [\"contract_id\", \"plan_id\", \"fips\"]\n",
    "\n",
    "        merged = enroll.merge(\n",
    "            sarea,\n",
    "            on=merge_keys,\n",
    "            how=\"inner\",\n",
    "        )\n",
    "\n",
    "        merged[\"year\"] = year\n",
    "        merged[\"month\"] = month\n",
    "        merged = flag_exclusions(merged)\n",
    "\n",
    "        group_keys = [\"contract_id\", \"plan_id\", \"fips\", \"year\"]\n",
    "        agg = {\n",
    "            \"enrollment\": \"sum\",\n",
    "            \"is_snp\": \"max\",\n",
    "            \"is_800_series\": \"max\",\n",
    "            \"is_pdp_only\": \"max\",\n",
    "            \"drop_hw2\": \"max\",\n",
    "        }\n",
    "        for c in meta_cols:\n",
    "            if c in merged.columns:\n",
    "                agg[c] = \"first\"\n",
    "\n",
    "        g = merged.groupby(group_keys, as_index=False).agg(agg)\n",
    "        g[\"months_observed\"] = 1\n",
    "        g[\"enroll_sum\"] = g[\"enrollment\"]\n",
    "        g.drop(columns=[\"enrollment\"], inplace=True)\n",
    "\n",
    "        g[\"dec_enrollment\"] = g[\"enroll_sum\"] if month == 12 else np.nan\n",
    "\n",
    "        if accum is None:\n",
    "            accum = g\n",
    "        else:\n",
    "            keys = [\"contract_id\", \"plan_id\", \"fips\", \"year\"]\n",
    "            accum = accum.merge(g, on=keys, how=\"outer\", suffixes=(\"\", \"_new\"))\n",
    "\n",
    "            for col in [\"enroll_sum\", \"months_observed\"]:\n",
    "                accum[col] = accum[col].fillna(0) + accum[f\"{col}_new\"].fillna(0)\n",
    "                accum.drop(columns=[f\"{col}_new\"], inplace=True)\n",
    "\n",
    "            for col in [\"is_snp\", \"is_800_series\", \"is_pdp_only\", \"drop_hw2\"]:\n",
    "                accum[col] = np.maximum(\n",
    "                    to_numeric_clean(accum[col]).fillna(0),\n",
    "                    to_numeric_clean(accum[f\"{col}_new\"]).fillna(0),\n",
    "                ).astype(int)\n",
    "                accum.drop(columns=[f\"{col}_new\"], inplace=True)\n",
    "\n",
    "            accum[\"dec_enrollment\"] = accum[\"dec_enrollment\"].combine_first(accum[\"dec_enrollment_new\"])\n",
    "            accum.drop(columns=[\"dec_enrollment_new\"], inplace=True)\n",
    "\n",
    "            for c in meta_cols:\n",
    "                if c in accum.columns and f\"{c}_new\" in accum.columns:\n",
    "                    accum[c] = accum[c].combine_first(accum[f\"{c}_new\"])\n",
    "                    accum.drop(columns=[f\"{c}_new\"], inplace=True)\n",
    "\n",
    "        del enroll_raw, sarea_raw, enroll, sarea, merged, g\n",
    "        gc.collect()\n",
    "\n",
    "    accum[\"avg_enrollment\"] = accum[\"enroll_sum\"] / accum[\"months_observed\"].replace({0: np.nan})\n",
    "\n",
    "    keep = [\"contract_id\", \"plan_id\", \"fips\", \"year\", \"avg_enrollment\", \"dec_enrollment\", \"months_observed\",\n",
    "            \"is_snp\", \"is_800_series\", \"is_pdp_only\", \"drop_hw2\"]\n",
    "    out = accum[keep].copy()\n",
    "\n",
    "    out.to_csv(out_path, index=False)\n",
    "    del accum, out\n",
    "    gc.collect()\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "586e69c0-6cb3-411e-a679-38ab28069d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building year 2014\n",
      "Building year 2015\n",
      "Building year 2016\n",
      "Building year 2017\n",
      "Building year 2018\n",
      "Building year 2019\n",
      "Wrote data/processed/plan_county_year_2014_2019.csv\n",
      "  contract_id plan_id   fips  year avg_enrollment dec_enrollment months_observed is_snp is_800_series is_pdp_only drop_hw2\n",
      "0       H0022     001  39023  2014           43.2           81.0            10.0      0             0           0        0\n",
      "1       H0022     001  39035  2014          264.1          344.0            10.0      0             0           0        0\n",
      "2       H0022     001  39051  2014           17.2           24.0            10.0      0             0           0        0\n",
      "3       H0022     001  39055  2014            0.0            0.0            10.0      0             0           0        0\n",
      "4       H0022     001  39057  2014           72.5          135.0            10.0      0             0           0        0\n"
     ]
    }
   ],
   "source": [
    "def build_plan_county_year_2014_2019_files() -> Path:\n",
    "    combined_path = OUT_DIR / \"plan_county_year_2014_2019.csv\"\n",
    "    if combined_path.exists():\n",
    "        return combined_path\n",
    "\n",
    "    wrote_header = False\n",
    "    for y in YEARS:\n",
    "        print(\"Building year\", y)\n",
    "        year_path = build_plan_county_year_one_year(y)\n",
    "\n",
    "        chunk = pd.read_csv(year_path, dtype=str)\n",
    "        chunk.to_csv(combined_path, mode=\"a\", header=not wrote_header, index=False)\n",
    "        wrote_header = True\n",
    "\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "\n",
    "    return combined_path\n",
    "\n",
    "combined_csv = build_plan_county_year_2014_2019_files()\n",
    "print(\"Wrote\", combined_csv)\n",
    "print(pd.read_csv(combined_csv, dtype=str, nrows=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a06fbf43-aba7-4bbd-b724-f8b6c934aff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exists: True bytes: 57910373\n",
      "first rows:\n",
      "  contract_id plan_id   fips  year      avg_enrollment dec_enrollment months_observed is_snp is_800_series is_pdp_only drop_hw2\n",
      "0       H0022     001  39023  2018   598.4166666666666          622.0            12.0      0             0           0        0\n",
      "1       H0022     001  39035  2018              3653.0         3657.0            12.0      0             0           0        0\n",
      "2       H0022     001  39051  2018  115.83333333333333          126.0            12.0      0             0           0        0\n",
      "3       H0022     001  39055  2018   77.33333333333333           80.0            12.0      0             0           0        0\n",
      "4       H0022     001  39057  2018   571.0833333333334          601.0            12.0      0             0           0        0\n",
      "lines: 1366488\n"
     ]
    }
   ],
   "source": [
    "y = 2018\n",
    "yp = OUT_DIR / f\"plan_county_year_{y}.csv\"\n",
    "print(\"exists:\", yp.exists(), \"bytes:\", yp.stat().st_size)\n",
    "\n",
    "print(\"first rows:\")\n",
    "print(pd.read_csv(yp, dtype=str, nrows=5))\n",
    "\n",
    "# count lines without loading into memory\n",
    "with open(yp, \"r\") as f:\n",
    "    n = sum(1 for _ in f)\n",
    "print(\"lines:\", n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47f77399-2242-4010-8b1c-823e04423df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined lines: 7774410\n",
      "  contract_id plan_id   fips  year avg_enrollment dec_enrollment months_observed is_snp is_800_series is_pdp_only drop_hw2\n",
      "0       H0022     001  39023  2014           43.2           81.0            10.0      0             0           0        0\n",
      "1       H0022     001  39035  2014          264.1          344.0            10.0      0             0           0        0\n",
      "2       H0022     001  39051  2014           17.2           24.0            10.0      0             0           0        0\n",
      "3       H0022     001  39055  2014            0.0            0.0            10.0      0             0           0        0\n",
      "4       H0022     001  39057  2014           72.5          135.0            10.0      0             0           0        0\n",
      "bytes: 329564529\n"
     ]
    }
   ],
   "source": [
    "combined_csv = OUT_DIR / \"plan_county_year_2014_2019.csv\"\n",
    "\n",
    "with open(combined_csv, \"r\") as f:\n",
    "    n = sum(1 for _ in f)\n",
    "print(\"combined lines:\", n)\n",
    "\n",
    "print(pd.read_csv(combined_csv, dtype=str, nrows=5))\n",
    "print(\"bytes:\", combined_csv.stat().st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0293ea7c-84cf-4ba5-8be9-e1d330ccea3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote data/processed/county_plan_counts_2014_2019.csv\n",
      "    fips  year plan_count\n",
      "0  01001  2014        386\n",
      "1  01003  2014        431\n",
      "2  01005  2014        370\n",
      "3  01007  2014        384\n",
      "4  01009  2014        387\n"
     ]
    }
   ],
   "source": [
    "PLAN_CY_PATH = OUT_DIR / \"plan_county_year_2014_2019.csv\"\n",
    "\n",
    "def build_county_plan_counts(in_path: Path) -> Path:\n",
    "    out_path = OUT_DIR / \"county_plan_counts_2014_2019.csv\"\n",
    "    if out_path.exists():\n",
    "        return out_path\n",
    "\n",
    "    parts = []\n",
    "    usecols = [\"fips\", \"year\", \"drop_hw2\"]\n",
    "\n",
    "    for chunk in pd.read_csv(in_path, dtype=str, usecols=usecols, chunksize=600_000):\n",
    "        drop = chunk[\"drop_hw2\"].astype(str).str.lower().str.strip().isin([\"1\",\"true\",\"t\",\"yes\",\"y\"])\n",
    "        chunk = chunk.loc[~drop, [\"fips\",\"year\"]].dropna()\n",
    "\n",
    "        tmp = chunk.groupby([\"fips\",\"year\"], as_index=False).size()\n",
    "        tmp = tmp.rename(columns={\"size\":\"plan_count\"})\n",
    "        parts.append(tmp)\n",
    "\n",
    "        del chunk, tmp\n",
    "        gc.collect()\n",
    "\n",
    "    out = pd.concat(parts, ignore_index=True)\n",
    "    out = out.groupby([\"fips\",\"year\"], as_index=False)[\"plan_count\"].sum()\n",
    "    out = out.sort_values([\"year\",\"fips\"]).reset_index(drop=True)\n",
    "    out.to_csv(out_path, index=False)\n",
    "    return out_path\n",
    "\n",
    "county_counts_path = build_county_plan_counts(PLAN_CY_PATH)\n",
    "print(\"Wrote\", county_counts_path)\n",
    "print(pd.read_csv(county_counts_path, dtype=str, nrows=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fca3ca3-12e3-4d1e-a5c8-02a045773e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19494, 4)\n",
      "    fips  ma_enrollment  eligibles  year\n",
      "0  01001         3000.0       9438  2014\n",
      "1  01003        13183.0      41640  2014\n",
      "2  01005          859.0       6004  2014\n",
      "3  01007         1665.0       4599  2014\n",
      "4  01009         4680.0      11193  2014\n"
     ]
    }
   ],
   "source": [
    "def read_penetration_year(year: int) -> pd.DataFrame:\n",
    "    path = find_year_file(PEN_EXTRACTED, year, \"penetration\")\n",
    "    df = read_csv_any(path)\n",
    "    df = normalize_columns(df)\n",
    "    df[\"fips\"] = coerce_fips(df)\n",
    "\n",
    "    cols = list(df.columns)\n",
    "    ma_col = first_existing_col(cols, [\"ma_enrollment\", \"ma_enroll\", \"enrolled\", \"enroll\"])\n",
    "    elig_col = first_existing_col(cols, [\"eligibles\", \"total_eligibles\", \"medicare_eligibles\", \"tot_eligibles\"])\n",
    "\n",
    "    if ma_col is None or elig_col is None:\n",
    "        raise KeyError(f\"Penetration file missing enrolled/eligibles columns. Columns: {sorted(cols)[:80]}\")\n",
    "\n",
    "    out = df[[\"fips\", ma_col, elig_col]].copy()\n",
    "    out = out.rename(columns={ma_col: \"ma_enrollment\", elig_col: \"eligibles\"})\n",
    "    out[\"ma_enrollment\"] = to_numeric_clean(out[\"ma_enrollment\"])\n",
    "    out[\"eligibles\"] = to_numeric_clean(out[\"eligibles\"])\n",
    "    out[\"year\"] = year\n",
    "    out = out.dropna(subset=[\"fips\"])\n",
    "    return out\n",
    "\n",
    "pen = pd.concat([read_penetration_year(y) for y in YEARS], ignore_index=True)\n",
    "print(pen.shape)\n",
    "print(pen.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20cd3e26-0a05-469c-9f45-c7a7f0ebb283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fips              object\n",
      "ma_enrollment    float64\n",
      "eligibles          int64\n",
      "year              object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "pen = pen.copy()\n",
    "pen[\"year\"] = pen[\"year\"].astype(str)\n",
    "pen[\"fips\"] = pen[\"fips\"].astype(str).str.zfill(5)\n",
    "print(pen.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c55ef996-a39f-43bc-80fb-8434662ffa09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote data/processed/county_hhi_ma_share_2014_2019.csv\n",
      "    fips  year ma_total_from_plans sumsq_enroll                  hhi ma_enrollment eligibles             ma_share\n",
      "0  01001  2014              3027.0    1223071.0  0.13348326683022055        3000.0    9438.0   0.3178639542275906\n",
      "1  01003  2014             13627.0   27941489.0  0.15046969271875732       13183.0   41640.0   0.3165946205571566\n",
      "2  01005  2014               863.0     252081.0   0.3384687064042676         859.0    6004.0  0.14307128580946035\n",
      "3  01007  2014              1661.0     484639.0  0.17566251443952183        1665.0    4599.0  0.36203522504892366\n",
      "4  01009  2014              4653.0    3412299.0  0.15760898558544553        4680.0   11193.0   0.4181184668989547\n"
     ]
    }
   ],
   "source": [
    "def build_county_hhi_ma_share(in_path: Path, pen: pd.DataFrame) -> Path:\n",
    "    out_path = OUT_DIR / \"county_hhi_ma_share_2014_2019.csv\"\n",
    "    if out_path.exists():\n",
    "        return out_path\n",
    "\n",
    "    parts = []\n",
    "    usecols = [\"fips\",\"year\",\"avg_enrollment\",\"dec_enrollment\",\"drop_hw2\"]\n",
    "\n",
    "    for chunk in pd.read_csv(in_path, dtype=str, usecols=usecols, chunksize=500_000):\n",
    "        drop = chunk[\"drop_hw2\"].astype(str).str.lower().str.strip().isin([\"1\",\"true\",\"t\",\"yes\",\"y\"])\n",
    "        chunk = chunk.loc[~drop].dropna(subset=[\"fips\",\"year\"])\n",
    "\n",
    "        chunk[\"dec_enrollment\"] = pd.to_numeric(chunk[\"dec_enrollment\"], errors=\"coerce\")\n",
    "        chunk[\"avg_enrollment\"] = pd.to_numeric(chunk[\"avg_enrollment\"], errors=\"coerce\")\n",
    "\n",
    "        e = chunk[\"dec_enrollment\"].fillna(chunk[\"avg_enrollment\"]).fillna(0.0)\n",
    "        chunk[\"e\"] = e\n",
    "        chunk[\"e2\"] = e * e\n",
    "\n",
    "        tmp = (\n",
    "            chunk.groupby([\"fips\",\"year\"], as_index=False)\n",
    "                 .agg(total_enroll=(\"e\",\"sum\"), sumsq_enroll=(\"e2\",\"sum\"))\n",
    "        )\n",
    "        parts.append(tmp)\n",
    "\n",
    "        del chunk, tmp\n",
    "        gc.collect()\n",
    "\n",
    "    out = pd.concat(parts, ignore_index=True)\n",
    "    out = out.groupby([\"fips\",\"year\"], as_index=False).agg(\n",
    "        total_enroll=(\"total_enroll\",\"sum\"),\n",
    "        sumsq_enroll=(\"sumsq_enroll\",\"sum\"),\n",
    "    )\n",
    "\n",
    "    out[\"hhi\"] = out[\"sumsq_enroll\"] / (out[\"total_enroll\"] * out[\"total_enroll\"])\n",
    "    out = out.rename(columns={\"total_enroll\":\"ma_total_from_plans\"})\n",
    "    out = out.merge(pen, on=[\"fips\",\"year\"], how=\"left\")\n",
    "    out[\"ma_share\"] = out[\"ma_enrollment\"] / out[\"eligibles\"]\n",
    "\n",
    "    out = out.sort_values([\"year\",\"fips\"]).reset_index(drop=True)\n",
    "    out.to_csv(out_path, index=False)\n",
    "    return out_path\n",
    "\n",
    "county_hhi_path = build_county_hhi_ma_share(PLAN_CY_PATH, pen)\n",
    "print(\"Wrote\", county_hhi_path)\n",
    "print(pd.read_csv(county_hhi_path, dtype=str, nrows=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91ae9b08-2bf9-451a-a8f4-d92d842109bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/processed/plan_county_year_2014_2019.csv exists True bytes 329564529\n",
      "data/processed/county_plan_counts_2014_2019.csv exists True bytes 290270\n",
      "data/processed/county_hhi_ma_share_2014_2019.csv exists True bytes 1484389\n"
     ]
    }
   ],
   "source": [
    "for p in [\n",
    "    OUT_DIR / \"plan_county_year_2014_2019.csv\",\n",
    "    OUT_DIR / \"county_plan_counts_2014_2019.csv\",\n",
    "    OUT_DIR / \"county_hhi_ma_share_2014_2019.csv\",\n",
    "]:\n",
    "    print(p, \"exists\", p.exists(), \"bytes\", p.stat().st_size if p.exists() else None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
