{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "973faf93-9ec2-4fa3-983a-cadf149125ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MA_ROOT: /scion/5261/econ470001/ma-data/ma\n",
      "CMSPAY_ZIP_DIR exists: True\n",
      "CMSPAY zip examples: ['2006paymentdata-.zip', '2007paymentdata-.zip', '2008paymentdata.zip', '2009paymentdata-.zip', '2010paymentdata-.zip', '2011paymentdata.zip', '2012paymentdata.zip', '2013paymentdata.zip', '2014paymentdata.zip', '2015-payment-data.zip', '2016paymentdata.zip', '2017paymentdata.zip', '2018paymentdata.zip', '2019paymentdata_0.zip', '2020Paymentdata_1_0.zip']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "import zipfile\n",
    "import shutil\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 180)\n",
    "\n",
    "YEARS = list(range(2014, 2020))\n",
    "\n",
    "CACHE_DIR = Path(\"data/cache\")\n",
    "OUT_DIR   = Path(\"data/processed\")\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CANDIDATE_MA_ROOTS = [\n",
    "    Path(\"/scion/5261/econ470001/ma-data/ma\"),\n",
    "    Path(\"/econ470/a0/work/ma-data/ma\"),\n",
    "    Path.cwd().parent / \"ma-data\" / \"ma\",\n",
    "]\n",
    "\n",
    "def pick_existing(paths: list[Path]) -> Path:\n",
    "    for p in paths:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(\"No candidate path exists:\\n\" + \"\\n\".join(map(str, paths)))\n",
    "\n",
    "MA_ROOT = pick_existing(CANDIDATE_MA_ROOTS)\n",
    "\n",
    "CMSPAY_ZIP_DIR = MA_ROOT / \"cms-payment\"\n",
    "if not CMSPAY_ZIP_DIR.exists():\n",
    "    raise FileNotFoundError(f\"cms-payment not found: {CMSPAY_ZIP_DIR}\")\n",
    "\n",
    "print(\"MA_ROOT:\", MA_ROOT)\n",
    "print(\"CMSPAY_ZIP_DIR exists:\", CMSPAY_ZIP_DIR.exists())\n",
    "print(\"CMSPAY zip examples:\", sorted([p.name for p in CMSPAY_ZIP_DIR.glob(\"*.zip\")])[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "472b3a71-e709-4727-98ff-f1a3f2a0fe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_colname(s: str) -> str:\n",
    "    s = str(s).strip().lower()\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-z0-9_]\", \"\", s)\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s\n",
    "\n",
    "def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [norm_colname(c) for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def clean_contract_id(x: pd.Series) -> pd.Series:\n",
    "    s = x.astype(str).str.strip()\n",
    "    s = s.replace({\"nan\": np.nan, \"none\": np.nan, \"None\": np.nan, \"\": np.nan})\n",
    "    s = s.str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "    return s\n",
    "\n",
    "def clean_plan_id(x: pd.Series) -> pd.Series:\n",
    "    s = x.astype(str).str.strip()\n",
    "    s = s.replace({\"nan\": np.nan, \"none\": np.nan, \"None\": np.nan, \"\": np.nan})\n",
    "    s = s.str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "    s = s.str.replace(r\"[^0-9]\", \"\", regex=True)\n",
    "    s = s.where(s.str.len() > 0, np.nan)\n",
    "    return s.str.zfill(3)\n",
    "\n",
    "def to_num(x: pd.Series) -> pd.Series:\n",
    "    s = x.astype(\"string\").str.strip()\n",
    "    s = s.replace({\"nan\": pd.NA, \"none\": pd.NA, \"None\": pd.NA, \"\": pd.NA, \"*\": pd.NA, \"$-\": pd.NA, \"-\": pd.NA})\n",
    "    s = s.str.replace(\",\", \"\", regex=False)\n",
    "    s = s.str.replace(\"$\", \"\", regex=False)\n",
    "    s = s.str.replace(\"%\", \"\", regex=False)\n",
    "    return pd.to_numeric(s, errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fda1b134-a945-4824-a916-b023cc64aa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_payment_zip(year: int) -> Path:\n",
    "    patterns = [\n",
    "        f\"{year}paymentdata*.zip\",\n",
    "        f\"{year}*payment*data*.zip\",\n",
    "        f\"{year}*payment*.zip\",\n",
    "        f\"*{year}*payment*data*.zip\",\n",
    "        f\"*{year}*payment*.zip\",\n",
    "    ]\n",
    "    hits = []\n",
    "    for pat in patterns:\n",
    "        hits.extend(list(CMSPAY_ZIP_DIR.glob(pat)))\n",
    "    hits = [p for p in hits if p.is_file()]\n",
    "    if not hits:\n",
    "        raise FileNotFoundError(f\"No payment zip found for {year} in {CMSPAY_ZIP_DIR}\")\n",
    "    hits = sorted(hits, key=lambda p: (len(p.name), p.name.lower()))\n",
    "    return hits[0]\n",
    "\n",
    "def year_dir_has_data(year_dir: Path) -> bool:\n",
    "    if not year_dir.exists():\n",
    "        return False\n",
    "    exts = {\".xlsx\", \".xls\", \".csv\", \".txt\", \".tsv\"}\n",
    "    files = [\n",
    "        p for p in year_dir.rglob(\"*\")\n",
    "        if p.is_file()\n",
    "        and p.suffix.lower() in exts\n",
    "        and not p.name.startswith(\"~$\")\n",
    "        and p.name.lower() != \"_extracted_ok.txt\"\n",
    "    ]\n",
    "    return len(files) > 0\n",
    "\n",
    "def extract_payment_zip(year: int) -> Path:\n",
    "    year_dir = CACHE_DIR / \"cms_payment_extracted\" / str(year)\n",
    "    ok_path = year_dir / \"_EXTRACTED_OK.txt\"\n",
    "\n",
    "    if year_dir_has_data(year_dir) and ok_path.exists():\n",
    "        return year_dir\n",
    "\n",
    "    zip_path = find_payment_zip(year)\n",
    "\n",
    "    if year_dir.exists():\n",
    "        shutil.rmtree(year_dir)\n",
    "    year_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "        z.extractall(year_dir)\n",
    "\n",
    "    if not year_dir_has_data(year_dir):\n",
    "        raise RuntimeError(f\"Extraction for {year} produced no readable data files. Zip was {zip_path}\")\n",
    "\n",
    "    ok_path.write_text(\"ok\\n\")\n",
    "    print(\"Extracted\", year, \"from\", zip_path.name, \"into\", year_dir)\n",
    "    return year_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4316cd0-2dc0-45b8-a491-9fe72b8bac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "\n",
    "def score_cols(cols: list[str]) -> int:\n",
    "    c = set([norm_colname(x) for x in cols])\n",
    "\n",
    "    contract_keys = {\"contract_id\", \"contract_number\", \"contract\"}\n",
    "    plan_keys = {\"plan_id\", \"pbp\", \"plan_benefit_package\", \"planbenefitpackage\"}\n",
    "\n",
    "    if not (any(k in c for k in contract_keys) and any(k in c for k in plan_keys)):\n",
    "        return -10\n",
    "\n",
    "    sc = 10\n",
    "    sc += 2 if (\"average_rebate_pmpm_payment\" in c or \"rebate\" in c) else 0\n",
    "    sc += 1 if (\"average_ab_pmpm_payment\" in c or \"payment\" in c) else 0\n",
    "    sc += 1 if (\"average_part_c_risk_score\" in c or \"average_part_risk_score\" in c or \"risk\" in c) else 0\n",
    "    return sc\n",
    "\n",
    "def read_sheet_as_table_openpyxl(path: Path, sheet_name: str, max_scan_rows: int = 250) -> pd.DataFrame | None:\n",
    "    wb = openpyxl.load_workbook(path, read_only=True, data_only=True)\n",
    "    if sheet_name not in wb.sheetnames:\n",
    "        return None\n",
    "    ws = wb[sheet_name]\n",
    "\n",
    "    rows = []\n",
    "    for i, row in enumerate(ws.iter_rows(values_only=True)):\n",
    "        if i >= max_scan_rows:\n",
    "            break\n",
    "        rows.append([(\"\" if v is None else str(v)) for v in row])\n",
    "\n",
    "    if len(rows) == 0:\n",
    "        return None\n",
    "\n",
    "    keys = [\"contract\", \"contract number\", \"pbp\", \"plan benefit\", \"plan_benefit\", \"rebate\", \"risk\", \"payment\"]\n",
    "\n",
    "    best_i = None\n",
    "    best_hits = -1\n",
    "    for i, r in enumerate(rows):\n",
    "        txt = \" \".join(r).lower()\n",
    "        hits = sum(k in txt for k in keys)\n",
    "        if hits > best_hits:\n",
    "            best_hits = hits\n",
    "            best_i = i\n",
    "\n",
    "    if best_i is None or best_hits <= 0:\n",
    "        return None\n",
    "\n",
    "    header = [norm_colname(x) for x in rows[best_i]]\n",
    "    header = [(\"col\" + str(j) if h == \"\" else h) for j, h in enumerate(header)]\n",
    "\n",
    "    data_rows = []\n",
    "    for row in ws.iter_rows(min_row=best_i + 2, values_only=True):\n",
    "        r = [(\"\" if v is None else str(v)) for v in row]\n",
    "        if all(x.strip() == \"\" for x in r):\n",
    "            continue\n",
    "        data_rows.append(r)\n",
    "\n",
    "    if len(data_rows) == 0:\n",
    "        return None\n",
    "\n",
    "    max_len = max(len(header), max(len(r) for r in data_rows))\n",
    "    header = header + [f\"col_extra_{j}\" for j in range(len(header), max_len)]\n",
    "    data_rows = [r + [\"\"] * (max_len - len(r)) for r in data_rows]\n",
    "\n",
    "    df = pd.DataFrame(data_rows, columns=header)\n",
    "    df = df.dropna(how=\"all\")\n",
    "    df = normalize_columns(df)\n",
    "\n",
    "    keep_cols = [c for c in df.columns if df[c].astype(str).str.strip().replace({\"nan\": \"\"}).ne(\"\").any()]\n",
    "    df = df[keep_cols]\n",
    "\n",
    "    if df.shape[0] == 0 or df.shape[1] == 0:\n",
    "        return None\n",
    "\n",
    "    return df\n",
    "\n",
    "def pick_best_sheet(path: Path) -> tuple[str, pd.DataFrame, int]:\n",
    "    xls = pd.ExcelFile(path, engine=\"openpyxl\")\n",
    "    best = None  # (score, sheet, df)\n",
    "\n",
    "    for sh in xls.sheet_names:\n",
    "        try:\n",
    "            df = read_sheet_as_table_openpyxl(path, sh)\n",
    "            if df is None:\n",
    "                continue\n",
    "            sc = score_cols(list(df.columns))\n",
    "            if best is None or sc > best[0]:\n",
    "                best = (sc, sh, df)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    if best is None:\n",
    "        raise RuntimeError(f\"Could not find a usable sheet/table in {path.name}\")\n",
    "\n",
    "    return best[1], best[2], best[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "784f148f-fd90-45d3-aae8-db96f1c39c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_partc_plan_workbook(year_dir: Path, year: int) -> Path:\n",
    "    xlsx = [p for p in year_dir.rglob(\"*.xlsx\") if p.is_file() and \"~$\" not in p.name]\n",
    "    if not xlsx:\n",
    "        raise FileNotFoundError(f\"No xlsx files in {year_dir}\")\n",
    "\n",
    "    y = str(year)\n",
    "    want = []\n",
    "    for p in xlsx:\n",
    "        n = p.name.lower()\n",
    "        if (y in p.name) and (\"partc\" in n or \"part_c\" in n) and (\"plan\" in n) and (\"level\" in n):\n",
    "            want.append(p)\n",
    "\n",
    "    if want:\n",
    "        want = sorted(want, key=lambda p: (len(p.name), p.name.lower()))\n",
    "        return want[0]\n",
    "\n",
    "    want2 = []\n",
    "    for p in xlsx:\n",
    "        n = p.name.lower()\n",
    "        if (y in p.name) and (\"partc\" in n or \"part_c\" in n) and (\"plan\" in n):\n",
    "            want2.append(p)\n",
    "\n",
    "    if want2:\n",
    "        want2 = sorted(want2, key=lambda p: (len(p.name), p.name.lower()))\n",
    "        return want2[0]\n",
    "\n",
    "    xlsx = sorted(xlsx, key=lambda p: (len(p.name), p.name.lower()))\n",
    "    return xlsx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceaede7c-8082-4f7a-8cb4-dee8ba4eba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_planlevel_payment(year: int) -> pd.DataFrame:\n",
    "    year_dir = extract_payment_zip(year)\n",
    "    wb = pick_partc_plan_workbook(year_dir, year)\n",
    "\n",
    "    sh, df, sc = pick_best_sheet(wb)\n",
    "    df = normalize_columns(df)\n",
    "    cols = list(df.columns)\n",
    "    cset = set(cols)\n",
    "\n",
    "    contract_col = \"contract_number\" if \"contract_number\" in cset else (\"contract_id\" if \"contract_id\" in cset else None)\n",
    "    plan_col = \"plan_benefit_package\" if \"plan_benefit_package\" in cset else (\"pbp\" if \"pbp\" in cset else (\"plan_id\" if \"plan_id\" in cset else None))\n",
    "\n",
    "    rebate_col = \"average_rebate_pmpm_payment\" if \"average_rebate_pmpm_payment\" in cset else (\"rebate\" if \"rebate\" in cset else None)\n",
    "    abpay_col  = \"average_ab_pmpm_payment\" if \"average_ab_pmpm_payment\" in cset else None\n",
    "    risk_col   = \"average_part_c_risk_score\" if \"average_part_c_risk_score\" in cset else (\"average_part_risk_score\" if \"average_part_risk_score\" in cset else None)\n",
    "\n",
    "    if contract_col is None or plan_col is None:\n",
    "        raise KeyError(f\"Missing contract or plan cols in {wb.name} sheet {sh}. Cols: {cols[:80]}\")\n",
    "    if rebate_col is None:\n",
    "        raise KeyError(f\"Missing rebate column in {wb.name} sheet {sh}. Cols: {cols[:80]}\")\n",
    "\n",
    "    out = pd.DataFrame()\n",
    "    out[\"contract_id\"] = clean_contract_id(df[contract_col])\n",
    "    out[\"plan_id\"]     = clean_plan_id(df[plan_col])\n",
    "    out[\"rebate_pmpm\"] = to_num(df[rebate_col])\n",
    "    out[\"ab_pmpm_payment\"] = to_num(df[abpay_col]) if abpay_col is not None else np.nan\n",
    "    out[\"risk_score\"] = to_num(df[risk_col]) if risk_col is not None else np.nan\n",
    "\n",
    "    out[\"year\"] = int(year)\n",
    "    out[\"source_file\"] = wb.name\n",
    "    out[\"source_sheet\"] = sh\n",
    "    out = out.dropna(subset=[\"contract_id\", \"plan_id\"])\n",
    "    out = out.drop_duplicates(subset=[\"contract_id\", \"plan_id\", \"year\"]).reset_index(drop=True)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a19a5f6-615a-4b7a-95a4-2d60e8dbb665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cms payment plan level: 2014\n",
      "  2014 rows 2825 file 2014PartCPlan Level.xlsx sheet result.srx\n",
      "Loading cms payment plan level: 2015\n",
      "  2015 rows 2742 file 2015PartCPlanLevel.xlsx sheet result.srx\n",
      "Loading cms payment plan level: 2016\n",
      "  2016 rows 2774 file 2016PartCPlanLevel.xlsx sheet 2016\n",
      "Loading cms payment plan level: 2017\n",
      "  2017 rows 2810 file 2017PartCPlanLevel.xlsx sheet 2017\n",
      "Loading cms payment plan level: 2018\n",
      "  2018 rows 3129 file 2018PartCPlanLevel.xlsx sheet result.srx\n",
      "Loading cms payment plan level: 2019\n",
      "  2019 rows 3617 file 2019PartCPlanLevel.xlsx sheet result.html\n",
      "cms_pay_planlevel shape: (17897, 8)\n",
      "Wrote: data/processed/cms_payment_planlevel_2014_2019.csv\n"
     ]
    }
   ],
   "source": [
    "pay_list = []\n",
    "for y in YEARS:\n",
    "    print(\"Loading cms payment plan level:\", y)\n",
    "    d = load_planlevel_payment(y)\n",
    "    print(\" \", y, \"rows\", d.shape[0], \"file\", d[\"source_file\"].iloc[0], \"sheet\", d[\"source_sheet\"].iloc[0])\n",
    "    pay_list.append(d)\n",
    "    gc.collect()\n",
    "\n",
    "cms_pay_planlevel = pd.concat(pay_list, ignore_index=True)\n",
    "print(\"cms_pay_planlevel shape:\", cms_pay_planlevel.shape)\n",
    "\n",
    "cms_pay_out = OUT_DIR / \"cms_payment_planlevel_2014_2019.csv\"\n",
    "cms_pay_planlevel.to_csv(cms_pay_out, index=False)\n",
    "print(\"Wrote:\", cms_pay_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bb2fd7d-6212-4b6d-a024-6c0b017cb030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bid nonmissing share: 1.0\n",
      "Ab payment nonmissing share: 1.0\n",
      "Rebate nonmissing share: 1.0\n",
      "Wrote: data/processed/plan_bids_pmpm_2014_2019.csv\n"
     ]
    }
   ],
   "source": [
    "b = cms_pay_planlevel.copy()\n",
    "b[\"ab_pmpm_payment\"] = pd.to_numeric(b[\"ab_pmpm_payment\"], errors=\"coerce\")\n",
    "b[\"rebate_pmpm\"] = pd.to_numeric(b[\"rebate_pmpm\"], errors=\"coerce\")\n",
    "\n",
    "b[\"bid_pmpm\"] = b[\"ab_pmpm_payment\"] - b[\"rebate_pmpm\"]\n",
    "\n",
    "print(\"Bid nonmissing share:\", b[\"bid_pmpm\"].notna().mean())\n",
    "print(\"Ab payment nonmissing share:\", b[\"ab_pmpm_payment\"].notna().mean())\n",
    "print(\"Rebate nonmissing share:\", b[\"rebate_pmpm\"].notna().mean())\n",
    "\n",
    "plan_bids_out = OUT_DIR / \"plan_bids_pmpm_2014_2019.csv\"\n",
    "b.to_csv(plan_bids_out, index=False)\n",
    "print(\"Wrote:\", plan_bids_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1bf70de-23dc-498c-9bcf-ade0c3e414d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: data/processed/plan_bids_pmpm_2014.csv rows 2825 bid nonmissing 1.0\n",
      "Wrote: data/processed/plan_bids_pmpm_2018.csv rows 3129 bid nonmissing 1.0\n"
     ]
    }
   ],
   "source": [
    "b2014 = b[b[\"year\"] == 2014].copy()\n",
    "b2018 = b[b[\"year\"] == 2018].copy()\n",
    "\n",
    "b2014_out = OUT_DIR / \"plan_bids_pmpm_2014.csv\"\n",
    "b2018_out = OUT_DIR / \"plan_bids_pmpm_2018.csv\"\n",
    "\n",
    "b2014.to_csv(b2014_out, index=False)\n",
    "b2018.to_csv(b2018_out, index=False)\n",
    "\n",
    "print(\"Wrote:\", b2014_out, \"rows\", len(b2014), \"bid nonmissing\", b2014[\"bid_pmpm\"].notna().mean())\n",
    "print(\"Wrote:\", b2018_out, \"rows\", len(b2018), \"bid nonmissing\", b2018[\"bid_pmpm\"].notna().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56e1251c-e1c5-4f42-b9bd-54f482bf7b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pc18 rows: (1366487, 12) merged rows: (1366487, 13) bid missing share: 0.6062465285070403\n"
     ]
    }
   ],
   "source": [
    "pc18_path = CACHE_DIR / \"plan_county_year_2018.csv\"\n",
    "if not pc18_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Missing {pc18_path}. Run hw2_enrollment first so this cache file exists.\"\n",
    "    )\n",
    "\n",
    "pc18 = pd.read_csv(pc18_path, dtype=str, low_memory=False)\n",
    "pc18 = normalize_columns(pc18)\n",
    "\n",
    "for c in [\"avg_enrollment\", \"dec_enrollment\", \"months_observed\", \"drop_hw2\"]:\n",
    "    if c in pc18.columns:\n",
    "        pc18[c] = pd.to_numeric(pc18[c], errors=\"coerce\")\n",
    "\n",
    "pc18[\"fips\"] = pc18[\"fips\"].astype(str).str.zfill(5)\n",
    "pc18[\"year\"] = pd.to_numeric(pc18[\"year\"], errors=\"coerce\").astype(int)\n",
    "pc18[\"contract_id\"] = clean_contract_id(pc18[\"contract_id\"])\n",
    "pc18[\"plan_id\"] = clean_plan_id(pc18[\"plan_id\"])\n",
    "\n",
    "pc18 = pc18[pc18[\"drop_hw2\"] != 1].copy()\n",
    "\n",
    "e = pc18[\"dec_enrollment\"].copy()\n",
    "e = e.fillna(pc18[\"avg_enrollment\"])\n",
    "e = e.fillna(0.0)\n",
    "pc18[\"enroll_for_weight\"] = e\n",
    "\n",
    "b18 = b[b[\"year\"] == 2018].copy()\n",
    "b18 = b18[[\"contract_id\", \"plan_id\", \"year\", \"bid_pmpm\"]].copy()\n",
    "b18[\"contract_id\"] = clean_contract_id(b18[\"contract_id\"])\n",
    "b18[\"plan_id\"] = clean_plan_id(b18[\"plan_id\"])\n",
    "b18[\"year\"] = pd.to_numeric(b18[\"year\"], errors=\"coerce\").astype(int)\n",
    "\n",
    "pc18m = pc18.merge(b18, on=[\"contract_id\", \"plan_id\", \"year\"], how=\"left\")\n",
    "\n",
    "print(\"pc18 rows:\", pc18.shape, \"merged rows:\", pc18m.shape, \"bid missing share:\", pc18m[\"bid_pmpm\"].isna().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d4d69dd-06af-42d1-9588-01e5f57406e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: data/processed/county_bid_2018.csv rows 3225\n"
     ]
    }
   ],
   "source": [
    "tmp = pc18m.dropna(subset=[\"bid_pmpm\"]).copy()\n",
    "\n",
    "tmp[\"w\"] = pd.to_numeric(tmp[\"enroll_for_weight\"], errors=\"coerce\").fillna(0.0)\n",
    "tmp[\"wx\"] = tmp[\"w\"] * pd.to_numeric(tmp[\"bid_pmpm\"], errors=\"coerce\")\n",
    "\n",
    "county_bid_2018 = tmp.groupby([\"fips\", \"year\"], as_index=False).agg(\n",
    "    n_plans=(\"bid_pmpm\", \"count\"),\n",
    "    bid_mean_unweighted=(\"bid_pmpm\", \"mean\"),\n",
    "    w_sum=(\"w\", \"sum\"),\n",
    "    wx_sum=(\"wx\", \"sum\"),\n",
    ")\n",
    "\n",
    "county_bid_2018[\"bid_mean_weighted\"] = county_bid_2018[\"wx_sum\"] / county_bid_2018[\"w_sum\"].replace({0: np.nan})\n",
    "\n",
    "county_bid_out = OUT_DIR / \"county_bid_2018.csv\"\n",
    "county_bid_2018.to_csv(county_bid_out, index=False)\n",
    "print(\"Wrote:\", county_bid_out, \"rows\", len(county_bid_2018))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a43dfd6d-153c-4304-8292-0f349a92fcec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: data/processed/county_bid_hhi_2018.csv rows 3225\n",
      "HHI cutoffs p33: 0.14846246259672544 p66: 0.23219048935982964\n",
      "market_type\n",
      "middle           1183\n",
      "uncompetitive    1036\n",
      "competitive      1006\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "hhi_path = Path(\"data/processed/county_hhi_ma_share_2014_2019.csv\")\n",
    "if not hhi_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing {hhi_path}. Run hw2_enrollment first.\")\n",
    "\n",
    "hhi = pd.read_csv(hhi_path, dtype=str, low_memory=False)\n",
    "hhi = normalize_columns(hhi)\n",
    "hhi[\"fips\"] = hhi[\"fips\"].astype(str).str.zfill(5)\n",
    "hhi[\"year\"] = pd.to_numeric(hhi[\"year\"], errors=\"coerce\").astype(int)\n",
    "hhi[\"hhi\"] = pd.to_numeric(hhi[\"hhi\"], errors=\"coerce\")\n",
    "\n",
    "hhi18 = hhi[hhi[\"year\"] == 2018].copy()\n",
    "\n",
    "merged18 = county_bid_2018.merge(hhi18[[\"fips\", \"year\", \"hhi\"]], on=[\"fips\", \"year\"], how=\"left\")\n",
    "\n",
    "p33 = merged18[\"hhi\"].quantile(0.33)\n",
    "p66 = merged18[\"hhi\"].quantile(0.66)\n",
    "\n",
    "merged18[\"market_type\"] = np.where(\n",
    "    merged18[\"hhi\"] <= p33,\n",
    "    \"competitive\",\n",
    "    np.where(merged18[\"hhi\"] >= p66, \"uncompetitive\", \"middle\")\n",
    ")\n",
    "\n",
    "out18 = OUT_DIR / \"county_bid_hhi_2018.csv\"\n",
    "merged18.to_csv(out18, index=False)\n",
    "\n",
    "print(\"Wrote:\", out18, \"rows\", len(merged18))\n",
    "print(\"HHI cutoffs p33:\", p33, \"p66:\", p66)\n",
    "print(merged18[\"market_type\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a480116-81ca-4e6e-a5fa-eaac56385d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook 1c outputs:\n",
      "  data/processed/cms_payment_planlevel_2014_2019.csv\n",
      "  data/processed/plan_bids_pmpm_2014_2019.csv\n",
      "  data/processed/plan_bids_pmpm_2014.csv\n",
      "  data/processed/plan_bids_pmpm_2018.csv\n",
      "  data/processed/county_bid_2018.csv\n",
      "  data/processed/county_bid_hhi_2018.csv\n",
      "Do not push:\n",
      "  data/cache/cms_payment_extracted\n"
     ]
    }
   ],
   "source": [
    "print(\"Notebook 1c outputs:\")\n",
    "print(\" \", OUT_DIR / \"cms_payment_planlevel_2014_2019.csv\")\n",
    "print(\" \", OUT_DIR / \"plan_bids_pmpm_2014_2019.csv\")\n",
    "print(\" \", OUT_DIR / \"plan_bids_pmpm_2014.csv\")\n",
    "print(\" \", OUT_DIR / \"plan_bids_pmpm_2018.csv\")\n",
    "print(\" \", OUT_DIR / \"county_bid_2018.csv\")\n",
    "print(\" \", OUT_DIR / \"county_bid_hhi_2018.csv\")\n",
    "\n",
    "print(\"Do not push:\")\n",
    "print(\" \", CACHE_DIR / \"cms_payment_extracted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
